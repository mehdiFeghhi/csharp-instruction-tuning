{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()  # e.g., returns 0\n",
    "current_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0+cu118\n",
      "CUDA available: True\n",
      "GPU name: Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla P40. Max memory = 22.413 GB.\n",
      "0.0 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk, Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def explode_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Explodes the \"text\" list in each example so that each element becomes its own example.\n",
    "    \"\"\"\n",
    "    new_examples = []\n",
    "    for example in dataset:\n",
    "        # example[\"text\"] is expected to be a list\n",
    "        for t in example[\"text\"]:\n",
    "            new_examples.append({\"text\": t})\n",
    "    return Dataset.from_list(new_examples)\n",
    "\n",
    "def main():\n",
    "    model_name = \"NousResearch/Hermes-3-Llama-3.1-8B\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    raw_datasets = load_from_disk('E:\\\\fine_tuning\\\\dataset_code')\n",
    "\n",
    "    system_prompt = (\n",
    "        \"\\n**Code Generation Request**\\n\\n\"\n",
    "        \" * Read the provided **Method Descriptions & Summaries**\\n\"\n",
    "        \" * **Complete the Body of Each Block code** based on the corresponding summaries\\n\"\n",
    "        \" * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\\n\"\n",
    "        \" * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\\n\"\n",
    "    )\n",
    "\n",
    "    def chunk_text(text, tokenizer, max_length=1024, stride=512):\n",
    "        \"\"\"\n",
    "        Tokenize the full text and then split it into overlapping chunks.\n",
    "        Each chunk is decoded back to a string.\n",
    "        \"\"\"\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), stride):\n",
    "            chunk_tokens = tokens[i: i + max_length]\n",
    "            chunk_str = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            chunks.append(chunk_str)\n",
    "        return chunks\n",
    "\n",
    "    def preprocess_example(batch):\n",
    "        \"\"\"\n",
    "        Process a batch of examples by creating sliding window chunks.\n",
    "        Each example's text is transformed into a list of chunk strings.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for prompt, response in zip(batch[\"prompt\"], batch[\"response\"]):\n",
    "            if isinstance(prompt, list):\n",
    "                prompt = \" \".join(prompt)\n",
    "            if isinstance(response, list):\n",
    "                response = \" \".join(response)\n",
    "            \n",
    "            full_text = (\n",
    "                \"<s>[INST] System: \" + system_prompt + \"\\n\"\n",
    "                \"User: \" + prompt.strip() + \" [/INST] \\n\"\n",
    "                \"Assistant: \" + response.strip() + \" </s>\"\n",
    "            )\n",
    "            \n",
    "            chunks = chunk_text(full_text, tokenizer, max_length=1024, stride=512)\n",
    "            all_chunks.append(chunks)\n",
    "        \n",
    "        return {\"text\": all_chunks}\n",
    "\n",
    "    # Process examples in batches.\n",
    "    processed_datasets = raw_datasets.map(preprocess_example, batched=True, num_proc=4)\n",
    "\n",
    "    # Explode the \"text\" list into individual examples for each split.\n",
    "    for split in processed_datasets.keys():\n",
    "        processed_datasets[split] = explode_dataset(processed_datasets[split])\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "    # Remove unnecessary columns.\n",
    "    remove_columns = processed_datasets[\"train\"].column_names\n",
    "    tokenized_datasets = processed_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "    print(\"Train dataset size:\", len(tokenized_datasets[\"train\"]))\n",
    "    print(\"Validation dataset size:\", len(tokenized_datasets[\"validation\"]))\n",
    "    if len(tokenized_datasets[\"validation\"]) > 0:\n",
    "        print(\"A sample validation example (tokenized):\")\n",
    "        print(tokenized_datasets[\"validation\"][0])\n",
    "    else:\n",
    "        print(\"Validation dataset is empty!\")\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    save_dir = \"./llama_finetuned_v4\"\n",
    "    print(\"Saving output to:\", save_dir)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_first_step=True,\n",
    "        eval_steps=1000,\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,  # Lower the eval batch size\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    if os.path.isdir(save_dir) and os.listdir(save_dir):\n",
    "        main_path = save_dir+\"/checkpoint-900\"\n",
    "        trainer.train(resume_from_checkpoint=main_path)\n",
    "    else:\n",
    "        # trainer.train()\n",
    "        raise\n",
    "            \n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(\"Evaluation metrics:\", eval_metrics)\n",
    "\n",
    "    model.save_pretrained(save_dir + \"_lora\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging LoRA adapters with base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172fe68c623f4673a1bbc87dc88bf7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e948a3e6995f4bf68fecf2c032891424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to./llama_finetuned_v4-merged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, get_peft_model, PeftModel  # Added PeftModel import\n",
    "\n",
    "def main():\n",
    "    # [Previous code remains identical until after model.save_pretrained()...]\n",
    "\n",
    "    save_dir = \"./llama_finetuned_v4\"\n",
    "\n",
    "    # New section: Merge LoRA with base model\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nMerging LoRA adapters with base model...\")\n",
    "    model_name = \"NousResearch/Hermes-3-Llama-3.1-8B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    # Reload base model in FP16 (without 4-bit quantization)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter weights\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        save_dir+\"_lora\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Merge and save\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(save_dir+\"-merged\")\n",
    "    tokenizer.save_pretrained(save_dir+\"-merged\")\n",
    "    \n",
    "    print(\"Merged model saved to\"+ save_dir +\"-merged\")\n",
    "    # --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find MSVC.\n",
      "WARNING: Failed to find Windows SDK.\n",
      "WARNING: Failed to find CUDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ba8ec5e9114340bcb9acf3f78c10bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35473965f52e471c8c9fd26dc60cc067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074f9c33f54a42eda074777c85ec34ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeBLEU Score for all examples: {'codebleu': 0.30083121073100166, 'ngram_match_score': 0.10495417518787843, 'weighted_ngram_match_score': 0.3367670204023581, 'syntax_match_score': 0.349162895806159, 'dataflow_match_score': 0.4124407515276112}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "def main():\n",
    "    # Specify the model path from Hugging Face\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Llama_v3_NousResearch_Hermes_instruct\\\\llama_finetuned_v4-merged\"\n",
    "\n",
    "    # Load the tokenizer and set the padding token (using eos_token if not already set)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Adjust the tokenizer padding side to \"right\"\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Load the base model in 4-bit mode using bitsandbytes for memory efficiency\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,      # requires bitsandbytes; lowers memory usage\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Additional model configurations\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    # Load test dataset and select only 10 examples\n",
    "    full_test_dataset = load_from_disk(\"E:\\\\fine_tuning\\\\dataset_code\")[\"test\"]\n",
    "    # test_dataset = full_test_dataset.select(range(10))\n",
    "    test_dataset = full_test_dataset\n",
    "    # Define system prompt\n",
    "    system_prompt = '''\n",
    "        **Code Generation Request** \n",
    "\n",
    "        * Read the provided **Method Descriptions & Summaries**\n",
    "        * **Complete the Body of Each Block code** based on the corresponding summaries\n",
    "        * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\n",
    "        * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\n",
    "    '''\n",
    "\n",
    "    # Preprocess function to combine the system prompt and user prompt into a single input string\n",
    "    def preprocess_example(example):\n",
    "        example[\"text\"] = (\n",
    "            \"<s>[INST] \"\n",
    "            \"System: \" + system_prompt + \"\\n\"\n",
    "            \"User: \" + example[\"prompt\"].strip() + \" [/INST] \\n\"\n",
    "            \"Assistant: \"\n",
    "        )\n",
    "        return example\n",
    "\n",
    "    # Preprocess test dataset\n",
    "    processed_test_dataset = test_dataset.map(preprocess_example)\n",
    "\n",
    "    # Tokenization function (using truncation to 2048 tokens for compatibility with fine-tuning)\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=2048, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_test_dataset = processed_test_dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=test_dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Generate predictions using max_new_tokens instead of max_length\n",
    "    def generate_code(example):\n",
    "        input_ids = tokenizer(example[\"text\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,  # Generate 512 new tokens after the prompt\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        example[\"generated_code\"] = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return example\n",
    "\n",
    "    generated_results = tokenized_test_dataset.map(generate_code)\n",
    "\n",
    "    # Evaluate with CodeBLEU on these 10 examples\n",
    "    references = [ex[\"response\"] for ex in test_dataset]\n",
    "    hypotheses = [ex[\"generated_code\"] for ex in generated_results]\n",
    "    # Compute CodeBLEU score\n",
    "    codebleu_score = calc_codebleu(\n",
    "        references,        # list of reference code (or list of lists if there are multiple references)\n",
    "        hypotheses,        # list of candidate code\n",
    "        lang=\"c_sharp\",           # specify the programming language\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),  # weights for n-gram, weighted n-gram, syntax, and data-flow matches\n",
    "        tokenizer=None           # if None, the default string split is used\n",
    "    )\n",
    "\n",
    "    print(\"CodeBLEU Score for all examples:\", codebleu_score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perplaxity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find MSVC.\n",
      "WARNING: Failed to find Windows SDK.\n",
      "WARNING: Failed to find CUDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7693e18e70d341daa7fc5e522c949320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6692d985b32439eb7b8b184bbcf258a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/7556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf2254dcb88485ab3c246016bd4cf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44a80f9e12142c29b511f77d3a73b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7556\n",
      "Validation dataset size: 636\n",
      "A sample validation example (tokenized):\n",
      "{'input_ids': [128000, 45147, 31868, 65562, 60, 744, 25, 720, 334, 2123, 24367, 6274, 57277, 353, 4557, 279, 3984, 3146, 3607, 3959, 25712, 612, 8279, 90433, 1035, 353, 3146, 12833, 279, 14285, 315, 9062, 8527, 2082, 334, 3196, 389, 279, 12435, 70022, 198, 353, 3146, 4152, 77038, 68063, 43287, 682, 8066, 3146, 2123, 46778, 334, 304, 24657, 1203, 36178, 320, 14196, 33981, 369, 24872, 92594, 198, 353, 3146, 39221, 7181, 68063, 40665, 1193, 279, 8308, 3146, 2123, 46778, 334, 2085, 93566, 6170, 477, 1495, 271, 1502, 25, 1066, 366, 2280, 397, 2640, 5000, 6700, 1345, 11864, 482, 1115, 4573, 374, 8647, 369, 11850, 30779, 6956, 304, 279, 3566, 5446, 11, 11951, 369, 1217, 17066, 323, 24645, 627, 2640, 694, 2280, 397, 2640, 366, 4789, 397, 2640, 549, 2735, 25711, 482, 1115, 538, 29972, 279, 17066, 1920, 369, 19957, 10339, 7540, 11, 23391, 430, 3932, 527, 19144, 1603, 32888, 2682, 5070, 627, 2640, 694, 4789, 1363, 2280, 5000, 6700, 1345, 11864, 1504, 262, 538, 549, 2735, 25711, 1504, 286, 1066, 366, 1743, 397, 286, 1066, 32982, 264, 502, 2937, 315, 279, 549, 2735, 25711, 538, 627, 286, 1066, 694, 1743, 397, 286, 1066, 366, 913, 836, 429, 3684, 891, 286, 1066, 1828, 320, 1939, 9566, 1680, 578, 1828, 30779, 3777, 304, 279, 1715, 15660, 627, 286, 1066, 694, 913, 397, 286, 1066, 366, 913, 836, 429, 1710, 891, 286, 1066, 2242, 320, 50, 708, 1502, 5368, 1710, 1680, 12499, 5110, 369, 279, 328, 708, 1502, 2937, 627, 286, 1066, 694, 913, 397, 286, 586, 549, 2735, 25711, 13248, 9566, 1828, 11, 328, 708, 1502, 5368, 1710, 2242, 8, 720, 286, 341, 310, 443, 15166, 220, 16, 25, 9307, 279, 1828, 30779, 3777, 304, 279, 721, 3684, 2115, 627, 310, 443, 15166, 220, 17, 25, 4324, 264, 502, 2937, 315, 328, 708, 1502, 1701, 279, 3984, 6683, 627, 286, 557, 286, 1066, 366, 1743, 397, 286, 1066, 63403, 459, 19957, 10339, 1715, 323, 12621, 369, 1217, 24645, 627, 286, 1066, 694, 1743, 397, 286, 1066, 366, 913, 836, 429, 1277, 2014, 891, 286, 1066, 1795, 2014, 320, 90711, 1680, 578, 2317, 315, 279, 1510, 10339, 1715, 627, 286, 1066, 694, 913, 397, 286, 1066, 366, 4310, 397, 286, 1066, 5546, 25, 39993, 279, 40107, 5784, 315, 8863, 279, 1715, 627, 286, 1066, 694, 4310, 397, 286, 586, 3393, 5546, 40767, 90193, 1795, 2014, 8, 720, 286, 341, 310, 443, 15166, 220, 16, 25, 7290, 279, 7030, 553, 1749, 315, 279, 549, 2735, 2937, 311, 1817, 422, 279, 1217, 374, 19144, 627, 310, 443, 15166, 220, 17, 25, 1442, 279, 1217, 374, 539, 19144, 11, 743, 279, 2077, 2704, 2082, 311, 220, 10841, 64202, 627, 310, 443, 15166, 220, 18, 25, 97295, 11, 6559, 279, 1217, 311, 279, 5982, 2199, 320, 6313, 291, 704, 304, 279, 2082, 4390, 310, 443, 15166, 220, 19, 25, 1442, 19144, 11, 1650, 279, 1828, 30779, 3777, 304, 279, 15660, 627, 286, 557, 286, 1066, 366, 13495, 397, 286, 1066, 482, 721, 3684, 320, 1939, 9566, 1680, 67084, 279, 5905, 311, 279, 1828, 30779, 304, 279, 15660, 627, 286, 1066, 482, 549, 2735, 320, 50, 708, 1502, 1680, 19840, 315, 328, 708, 1502, 1511, 369, 11850, 1217, 17066, 627, 286, 1066, 694, 13495, 397, 262, 457, 92, 66028, 65562, 60, 720, 72803, 25, 55375, 66, 54017, 198, 985, 9708, 28098, 280, 985, 5210, 17175, 16015, 280, 985, 5210, 17175, 9715, 280, 985, 5210, 21167, 22968, 280, 985, 744, 7653, 8490, 280, 985, 549, 2735, 401, 2280, 5000, 6700, 1345, 11864, 198, 517, 262, 443, 1472, 1253, 1205, 311, 4685, 279, 5210, 17175, 9715, 95070, 6462, 1139, 701, 2447, 198, 262, 586, 538, 549, 2735, 25711, 198, 262, 341, 286, 879, 5881, 6274, 9566, 721, 3684, 401, 286, 879, 5881, 328, 708, 1502, 549, 2735, 280, 286, 586, 549, 2735, 25711, 13248, 9566, 1828, 11, 328, 708, 1502, 5368, 1710, 2242, 340, 286, 341, 310, 721, 3684, 284, 1828, 280, 310, 549, 2735, 284, 502, 328, 708, 1502, 8928, 317, 286, 457, 286, 586, 3393, 5546, 40767, 90193, 1795, 2014, 340, 286, 341, 310, 422, 1533, 52, 2735, 34958, 553, 20480, 2014, 1192, 310, 341, 394, 443, 1795, 2014, 12859, 39969, 12597, 2735, 22793, 6363, 18195, 20480, 2014, 9856, 30940, 6298, 11, 1795, 2014, 9856, 17932, 705, 837, 317, 394, 1795, 2014, 12859, 38928, 284, 8266, 21541, 10761, 10841, 52281, 280, 394, 471, 280, 310, 17398, 310, 2597, 721, 3684, 20480, 2014, 8295, 286, 457, 262, 4555, 633, 74694, 694, 82, 29, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040, 128040], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "./llama_finetuned_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3778' max='3778' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3778/3778 23:11:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.215900</td>\n",
       "      <td>0.349948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.315027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.298254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='636' max='636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [636/636 25:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': 0.2921094298362732, 'eval_runtime': 1549.0725, 'eval_samples_per_second': 0.411, 'eval_steps_per_second': 0.411, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk, Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def explode_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Explodes the \"text\" list in each example so that each element becomes its own example.\n",
    "    \"\"\"\n",
    "    new_examples = []\n",
    "    for example in dataset:\n",
    "        # example[\"text\"] is expected to be a list\n",
    "        for t in example[\"text\"]:\n",
    "            new_examples.append({\"text\": t})\n",
    "    return Dataset.from_list(new_examples)\n",
    "def main():\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Llama_v3_NousResearch_Hermes_instruct\\\\llama_finetuned_v4-merged\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    raw_datasets = load_from_disk('E:\\\\fine_tuning\\\\dataset_code_bank')\n",
    "\n",
    "    system_prompt = (\n",
    "        \"\\n**Code Generation Request**\\n\\n\"\n",
    "        \" * Read the provided **Method Descriptions & Summaries**\\n\"\n",
    "        \" * **Complete the Body of Each Block code** based on the corresponding summaries\\n\"\n",
    "        \" * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\\n\"\n",
    "        \" * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\\n\"\n",
    "    )\n",
    "\n",
    "    def chunk_text(text, tokenizer, max_length=1024, stride=512):\n",
    "        \"\"\"\n",
    "        Tokenize the full text and then split it into overlapping chunks.\n",
    "        Each chunk is decoded back to a string.\n",
    "        \"\"\"\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), stride):\n",
    "            chunk_tokens = tokens[i: i + max_length]\n",
    "            chunk_str = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            chunks.append(chunk_str)\n",
    "        return chunks\n",
    "\n",
    "    def preprocess_example(batch):\n",
    "        \"\"\"\n",
    "        Process a batch of examples by creating sliding window chunks.\n",
    "        Each example's text is transformed into a list of chunk strings.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for prompt, response in zip(batch[\"prompt\"], batch[\"response\"]):\n",
    "            if isinstance(prompt, list):\n",
    "                prompt = \" \".join(prompt)\n",
    "            if isinstance(response, list):\n",
    "                response = \" \".join(response)\n",
    "            \n",
    "            full_text = (\n",
    "                \"<s>[INST] System: \" + system_prompt + \"\\n\"\n",
    "                \"User: \" + prompt.strip() + \" [/INST] \\n\"\n",
    "                \"Assistant: \" + response.strip() + \" </s>\"\n",
    "            )\n",
    "            \n",
    "            chunks = chunk_text(full_text, tokenizer, max_length=1024, stride=512)\n",
    "            all_chunks.append(chunks)\n",
    "        \n",
    "        return {\"text\": all_chunks}\n",
    "\n",
    "    # Process examples in batches.\n",
    "    processed_datasets = raw_datasets.map(preprocess_example, batched=True, num_proc=4)\n",
    "\n",
    "    # Explode the \"text\" list into individual examples for each split.\n",
    "    for split in processed_datasets.keys():\n",
    "        processed_datasets[split] = explode_dataset(processed_datasets[split])\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "    # Remove unnecessary columns.\n",
    "    remove_columns = processed_datasets[\"train\"].column_names\n",
    "    tokenized_datasets = processed_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "    print(\"Train dataset size:\", len(tokenized_datasets[\"train\"]))\n",
    "    print(\"Validation dataset size:\", len(tokenized_datasets[\"validation\"]))\n",
    "    if len(tokenized_datasets[\"validation\"]) > 0:\n",
    "        print(\"A sample validation example (tokenized):\")\n",
    "        print(tokenized_datasets[\"validation\"][0])\n",
    "    else:\n",
    "        print(\"Validation dataset is empty!\")\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "    save_dir = \"./llama_finetuned_v5\"\n",
    "    \n",
    "    print(save_dir)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_first_step=True,\n",
    "        eval_steps=1000,\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,  # Lower the eval batch size\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(\"Evaluation metrics:\", eval_metrics)\n",
    "\n",
    "    model.save_pretrained(save_dir+\"_lora\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging LoRA adapters with base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a305b078ab754040b25696b13798bab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3109087131304c9c94ce8fc7b7b01257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to./llama_finetuned_v5-merged\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import  PeftModel  # Added PeftModel import\n",
    "\n",
    "def main():\n",
    "    # [Previous code remains identical until after model.save_pretrained()...]\n",
    "\n",
    "    save_dir =  \"./llama_finetuned_v5\"\n",
    "\n",
    "    # New section: Merge LoRA with base model\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nMerging LoRA adapters with base model...\")\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Llama_v3_NousResearch_Hermes_instruct\\\\llama_finetuned_v4-merged\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    # Reload base model in FP16 (without 4-bit quantization)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter weights\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        save_dir+\"_lora\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Merge and save\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(save_dir+\"-merged\")\n",
    "    tokenizer.save_pretrained(save_dir+\"-merged\")\n",
    "    \n",
    "    print(\"Merged model saved to\"+ save_dir +\"-merged\")\n",
    "    # --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac973a7308b14034bf5b1ee8ba7413a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06cb3ae672d4001b305419b4bf3c48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34d6e7770a146558c08857dfc6f6224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeBLEU Score for all examples: {'codebleu': 0.28129948907018143, 'ngram_match_score': 0.09920597103349899, 'weighted_ngram_match_score': 0.2274880709910401, 'syntax_match_score': 0.6583697714710007, 'dataflow_match_score': 0.14013414278518596}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "def main():\n",
    "    # Specify the model path from Hugging Face\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Llama_v3_NousResearch_Hermes_instruct\\\\llama_finetuned_v5-merged\"\n",
    "\n",
    "    # Load the tokenizer and set the padding token (using eos_token if not already set)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Adjust the tokenizer padding side to \"right\"\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Load the base model in 4-bit mode using bitsandbytes for memory efficiency\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,      # requires bitsandbytes; lowers memory usage\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Additional model configurations\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    # Load test dataset and select only 10 examples\n",
    "    full_test_dataset = load_from_disk(\"E:\\\\fine_tuning\\\\dataset_code_bank\")[\"test\"]\n",
    "    # test_dataset = full_test_dataset.select(range(10))\n",
    "    test_dataset = full_test_dataset\n",
    "    # Define system prompt\n",
    "    system_prompt = '''\n",
    "        **Code Generation Request** \n",
    "\n",
    "        * Read the provided **Method Descriptions & Summaries**\n",
    "        * **Complete the Body of Each Block code** based on the corresponding summaries\n",
    "        * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\n",
    "        * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\n",
    "    '''\n",
    "\n",
    "    # Preprocess function to combine the system prompt and user prompt into a single input string\n",
    "    def preprocess_example(example):\n",
    "        example[\"text\"] = (\n",
    "            \"<s>[INST] \"\n",
    "            \"System: \" + system_prompt + \"\\n\"\n",
    "            \"User: \" + example[\"prompt\"].strip() + \" [/INST] \\n\"\n",
    "            \"Assistant: \"\n",
    "        )\n",
    "        return example\n",
    "\n",
    "    # Preprocess test dataset\n",
    "    processed_test_dataset = test_dataset.map(preprocess_example)\n",
    "\n",
    "    # Tokenization function (using truncation to 2048 tokens for compatibility with fine-tuning)\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=2048, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_test_dataset = processed_test_dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=test_dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Generate predictions using max_new_tokens instead of max_length\n",
    "    def generate_code(example):\n",
    "        input_ids = tokenizer(example[\"text\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,  # Generate 512 new tokens after the prompt\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        example[\"generated_code\"] = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return example\n",
    "\n",
    "    generated_results = tokenized_test_dataset.map(generate_code)\n",
    "\n",
    "    # Evaluate with CodeBLEU on these 10 examples\n",
    "    references = [ex[\"response\"] for ex in test_dataset]\n",
    "    hypotheses = [ex[\"generated_code\"] for ex in generated_results]\n",
    "    # Compute CodeBLEU score\n",
    "    codebleu_score = calc_codebleu(\n",
    "        references,        # list of reference code (or list of lists if there are multiple references)\n",
    "        hypotheses,        # list of candidate code\n",
    "        lang=\"c_sharp\",           # specify the programming language\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),  # weights for n-gram, weighted n-gram, syntax, and data-flow matches\n",
    "        tokenizer=None           # if None, the default string split is used\n",
    "    )\n",
    "\n",
    "    print(\"CodeBLEU Score for all examples:\", codebleu_score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
