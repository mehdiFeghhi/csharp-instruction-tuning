{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_device = torch.cuda.current_device()  # e.g., returns 0\n",
    "current_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0+cu118\n",
      "CUDA available: True\n",
      "GPU name: Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla P40. Max memory = 22.413 GB.\n",
      "0.0 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find MSVC.\n",
      "WARNING: Failed to find Windows SDK.\n",
      "WARNING: Failed to find CUDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da42c431dcf34e7987eba9e05b4489d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d545163a4248f690dcd2a4d2e5305e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/7841 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4365c3ad67284996a8f9bfcdf00b82bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2028 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d0ba58203d448d99f54ccb5e59d305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2380 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7841\n",
      "Validation dataset size: 2028\n",
      "A sample validation example (tokenized):\n",
      "{'input_ids': [8948, 271, 334, 2078, 23470, 6145, 56177, 353, 4457, 279, 3897, 3070, 3523, 3874, 24685, 609, 8116, 89333, 1019, 353, 3070, 12548, 279, 13958, 315, 8886, 8362, 2038, 334, 3118, 389, 279, 12159, 68922, 198, 353, 3070, 4061, 75938, 66963, 42187, 678, 7907, 3070, 2078, 45678, 334, 304, 23725, 1182, 35078, 320, 13874, 32881, 369, 23922, 91494, 198, 353, 3070, 38121, 7036, 66963, 39565, 1172, 279, 8145, 3070, 2078, 45678, 334, 2041, 92466, 6042, 476, 1467, 271, 872, 198, 2575, 366, 2231, 397, 2575, 5100, 92967, 1963, 54097, 481, 1096, 4473, 374, 8480, 369, 8241, 19721, 311, 10091, 24083, 304, 279, 92450, 3766, 12626, 624, 2575, 690, 2231, 397, 2575, 366, 4684, 397, 2575, 92450, 39949, 6482, 31282, 481, 1096, 536, 5707, 8894, 5413, 369, 7842, 323, 18150, 1651, 24083, 304, 279, 92450, 3766, 624, 2575, 690, 4684, 1339, 2231, 5100, 92967, 1963, 54097, 1476, 262, 1099, 7130, 536, 92450, 39949, 6482, 31282, 1476, 286, 1048, 366, 1708, 397, 286, 1048, 24475, 5248, 24083, 311, 279, 24083, 4426, 624, 286, 1048, 690, 1708, 397, 286, 1048, 366, 903, 829, 428, 55627, 6482, 881, 286, 1048, 24083, 6482, 320, 1791, 64, 1963, 39949, 6482, 1648, 576, 4426, 311, 892, 24083, 686, 387, 3694, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 903, 829, 428, 55627, 881, 286, 1048, 24083, 320, 8517, 35506, 11, 3990, 37023, 362, 10997, 12731, 1651, 4494, 311, 862, 12159, 7013, 4494, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 4216, 397, 286, 1048, 6517, 64, 1963, 39949, 6482, 25, 5184, 279, 6049, 24083, 4426, 624, 286, 1048, 690, 4216, 397, 286, 584, 1099, 6517, 64, 1963, 39949, 6482, 2691, 39949, 70676, 64, 1963, 39949, 6482, 24083, 6482, 11, 10466, 35506, 11, 3990, 29, 24083, 8, 715, 286, 341, 310, 442, 14822, 220, 16, 25, 54340, 1526, 1817, 1376, 19083, 6716, 304, 279, 24083, 10997, 624, 310, 442, 14822, 220, 17, 25, 1752, 1817, 7013, 11, 912, 432, 438, 264, 40558, 2473, 311, 279, 24083, 4426, 624, 310, 442, 13023, 14822, 25, 3411, 279, 6049, 24083, 4426, 624, 286, 555, 286, 1048, 366, 1708, 397, 286, 1048, 24475, 264, 3175, 7013, 311, 279, 24083, 4426, 624, 286, 1048, 690, 1708, 397, 286, 1048, 366, 903, 829, 428, 55627, 6482, 881, 286, 1048, 24083, 6482, 320, 1791, 64, 1963, 39949, 6482, 1648, 576, 4426, 311, 892, 279, 7013, 686, 387, 3694, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 903, 829, 428, 1050, 929, 881, 286, 1048, 53081, 320, 929, 1648, 576, 943, 315, 279, 1651, 369, 892, 279, 7013, 374, 1660, 3694, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 903, 829, 428, 17905, 929, 881, 286, 1048, 7013, 929, 320, 929, 1648, 576, 943, 315, 279, 7013, 311, 387, 5815, 448, 279, 1651, 943, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 4216, 397, 286, 1048, 6517, 64, 1963, 39949, 6482, 25, 5184, 279, 6049, 24083, 4426, 624, 286, 1048, 690, 4216, 397, 286, 584, 1099, 6517, 64, 1963, 39949, 6482, 2691, 3050, 70676, 64, 1963, 39949, 6482, 24083, 6482, 11, 3990, 53081, 11, 3990, 7013, 929, 8, 715, 286, 341, 310, 442, 14822, 220, 16, 25, 2691, 279, 5189, 7013, 438, 264, 40558, 2473, 369, 279, 2661, 1651, 943, 624, 310, 442, 13023, 14822, 25, 3411, 279, 6049, 24083, 4426, 624, 286, 555, 286, 1048, 366, 1708, 397, 286, 1048, 24475, 264, 13954, 7013, 311, 279, 24083, 4426, 624, 286, 1048, 690, 1708, 397, 286, 1048, 366, 903, 829, 428, 55627, 6482, 881, 286, 1048, 24083, 6482, 320, 1791, 64, 1963, 39949, 6482, 1648, 576, 4426, 311, 892, 279, 7013, 686, 387, 3694, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 4216, 397, 286, 1048, 6517, 64, 1963, 39949, 6482, 25, 5184, 279, 6049, 24083, 4426, 624, 286, 1048, 690, 4216, 397, 286, 584, 1099, 6517, 64, 1963, 39949, 6482, 2691, 3050, 3125, 929, 11, 350, 929, 6750, 2235, 1791, 64, 1963, 39949, 6482, 24083, 6482, 340, 310, 1380, 350, 929, 549, 358, 851, 198, 310, 1380, 350, 929, 6750, 549, 358, 851, 3050, 198, 286, 341, 310, 442, 14822, 220, 16, 25, 2691, 279, 13954, 7013, 438, 264, 40558, 2473, 369, 279, 5189, 1651, 943, 624, 310, 442, 13023, 14822, 25, 3411, 279, 6049, 24083, 4426, 624, 286, 555, 286, 1048, 366, 1708, 397, 286, 1048, 350, 4019, 311, 912, 5248, 24083, 311, 279, 24083, 4426, 421, 807, 653, 537, 2669, 3000, 624, 286, 1048, 690, 1708, 397, 286, 1048, 366, 903, 829, 428, 55627, 6482, 881, 286, 1048, 24083, 6482, 320, 1791, 64, 1963, 39949, 6482, 1648, 576, 4426, 311, 892, 24083, 686, 387, 3694, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 903, 829, 428, 55627, 881, 286, 1048, 24083, 320, 8517, 35506, 11, 3990, 37023, 362, 10997, 12731, 1651, 4494, 311, 862, 12159, 7013, 4494, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 4216, 397, 286, 1048, 6517, 64, 1963, 39949, 6482, 25, 5184, 279, 6049, 24083, 4426, 624, 286, 1048, 690, 4216, 397, 286, 584, 1099, 6517, 64, 1963, 39949, 6482, 9735, 2212, 39949, 70676, 64, 1963, 39949, 6482, 24083, 6482, 11, 10466, 35506, 11, 3990, 29, 24083, 8, 715, 286, 341, 310, 442, 14822, 220, 16, 25, 54340, 1526, 1817, 1376, 19083, 6716, 304, 279, 24083, 10997, 624, 310, 442, 14822, 220, 17, 25, 1752, 1817, 7013, 11, 1430, 311, 912, 432, 438, 264, 40558, 2473, 311, 279, 24083, 4426, 624, 310, 442, 13023, 14822, 25, 3411, 279, 6049, 24083, 4426, 624, 286, 555, 286, 1048, 366, 1708, 397, 286, 1048, 350, 4019, 311, 912, 264, 3175, 7013, 311, 279, 24083, 4426, 421, 432, 1558, 537, 2669, 3000, 624, 286, 1048, 690, 1708, 397, 286, 1048, 366, 903, 829, 428, 55627, 6482, 881, 286, 1048, 24083, 6482, 320, 1791, 64, 1963, 39949, 6482, 1648, 576, 4426, 311, 892, 279, 7013, 686, 387, 3694, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 903, 829, 428, 1050, 929, 881, 286, 1048, 53081, 320, 929, 1648, 576, 943, 315, 279, 1651, 369, 892, 279, 7013, 374, 1660, 3694, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 903, 829, 428, 17905, 929, 881, 286, 151643, 151643, 151643, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}\n",
      "Saving output to: ./Qwen_finetuned_v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5880' max='5880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5880/5880 49:03:19, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.581400</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.553000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.511400</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2028' max='2028' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2028/2028 1:48:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': nan, 'eval_runtime': 6539.9675, 'eval_samples_per_second': 0.31, 'eval_steps_per_second': 0.31, 'epoch': 2.9988521872210177}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk, Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def explode_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Explodes the \"text\" list in each example so that each element becomes its own example.\n",
    "    \"\"\"\n",
    "    new_examples = []\n",
    "    for example in dataset:\n",
    "        # example[\"text\"] is expected to be a list\n",
    "        for t in example[\"text\"]:\n",
    "            new_examples.append({\"text\": t})\n",
    "    return Dataset.from_list(new_examples)\n",
    "\n",
    "def main():\n",
    "    model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    raw_datasets = load_from_disk('E:\\\\fine_tuning\\\\dataset_code')\n",
    "\n",
    "    system_prompt = (\n",
    "        \"\\n**Code Generation Request**\\n\\n\"\n",
    "        \" * Read the provided **Method Descriptions & Summaries**\\n\"\n",
    "        \" * **Complete the Body of Each Block code** based on the corresponding summaries\\n\"\n",
    "        \" * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\\n\"\n",
    "        \" * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\\n\"\n",
    "    )\n",
    "\n",
    "    def chunk_text(text, tokenizer, max_length=1024, stride=512):\n",
    "        \"\"\"\n",
    "        Tokenize the full text and then split it into overlapping chunks.\n",
    "        Each chunk is decoded back to a string.\n",
    "        \"\"\"\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), stride):\n",
    "            chunk_tokens = tokens[i: i + max_length]\n",
    "            chunk_str = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            chunks.append(chunk_str)\n",
    "        return chunks\n",
    "\n",
    "    def preprocess_example(batch):\n",
    "        \"\"\"\n",
    "        Process a batch of examples by creating sliding window chunks.\n",
    "        Each example's text is transformed into a list of chunk strings.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for prompt, response in zip(batch[\"prompt\"], batch[\"response\"]):\n",
    "            if isinstance(prompt, list):\n",
    "                prompt = \" \".join(prompt)\n",
    "            if isinstance(response, list):\n",
    "                response = \" \".join(response)\n",
    "            \n",
    "            full_text = (\n",
    "            \"<|im_start|>system\\n\" + system_prompt + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\" + prompt.strip() + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\" + response.strip() + \"<|im_end|>\\n\"\n",
    "        )\n",
    "            \n",
    "            chunks = chunk_text(full_text, tokenizer, max_length=1024, stride=512)\n",
    "            all_chunks.append(chunks)\n",
    "        \n",
    "        return {\"text\": all_chunks}\n",
    "\n",
    "    # Process examples in batches.\n",
    "    processed_datasets = raw_datasets.map(preprocess_example, batched=True, num_proc=4)\n",
    "\n",
    "    # Explode the \"text\" list into individual examples for each split.\n",
    "    for split in processed_datasets.keys():\n",
    "        processed_datasets[split] = explode_dataset(processed_datasets[split])\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "    # Remove unnecessary columns.\n",
    "    remove_columns = processed_datasets[\"train\"].column_names\n",
    "    tokenized_datasets = processed_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "    print(\"Train dataset size:\", len(tokenized_datasets[\"train\"]))\n",
    "    print(\"Validation dataset size:\", len(tokenized_datasets[\"validation\"]))\n",
    "    if len(tokenized_datasets[\"validation\"]) > 0:\n",
    "        print(\"A sample validation example (tokenized):\")\n",
    "        print(tokenized_datasets[\"validation\"][0])\n",
    "    else:\n",
    "        print(\"Validation dataset is empty!\")\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    save_dir = \"./Qwen_finetuned_v4\"\n",
    "    print(\"Saving output to:\", save_dir)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_first_step=True,\n",
    "        eval_steps=1000,\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,  # Lower the eval batch size\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    if os.path.isdir(save_dir) and os.listdir(save_dir):\n",
    "    #     main_path = save_dir+\"/checkpoint-900\"\n",
    "    #     trainer.train(resume_from_checkpoint=main_path)\n",
    "    # else:\n",
    "        trainer.train()\n",
    "        # raise\n",
    "            \n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(\"Evaluation metrics:\", eval_metrics)\n",
    "\n",
    "    model.save_pretrained(save_dir + \"_lora\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging LoRA adapters with base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5fb30750944a93a16914e2809f577c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae4e02295134b0c96a372d3d40fd2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to./Qwen_finetuned_v4-merged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, get_peft_model, PeftModel  # Added PeftModel import\n",
    "\n",
    "def main():\n",
    "    # [Previous code remains identical until after model.save_pretrained()...]\n",
    "\n",
    "    save_dir = \"./Qwen_finetuned_v4\"\n",
    "\n",
    "    # New section: Merge LoRA with base model\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nMerging LoRA adapters with base model...\")\n",
    "    model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    # Reload base model in FP16 (without 4-bit quantization)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter weights\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        save_dir+\"_lora\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Merge and save\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(save_dir+\"-merged\")\n",
    "    tokenizer.save_pretrained(save_dir+\"-merged\")\n",
    "    \n",
    "    print(\"Merged model saved to\"+ save_dir +\"-merged\")\n",
    "    # --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find MSVC.\n",
      "WARNING: Failed to find Windows SDK.\n",
      "WARNING: Failed to find CUDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f685c4ff247439c87f425fb296fc513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440af8f6c4c445f6b72f80ff438db1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e78f4739b734f9cab5e0e48fb2566f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1770c7e9799945769ad4bed1b56328c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeBLEU Score for all examples: {'codebleu': 0.31630421135449427, 'ngram_match_score': 0.11803688139341223, 'weighted_ngram_match_score': 0.356302488844282, 'syntax_match_score': 0.36877137044611563, 'dataflow_match_score': 0.4221061047341671}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "def main():\n",
    "    # Specify the model path from Hugging Face\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Qwen2.5-Coder-7B-Instruct\\\\Qwen_finetuned_v4-merged\"\n",
    "\n",
    "    # Load the tokenizer and set the padding token (using eos_token if not already set)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Adjust the tokenizer padding side to \"right\"\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Load the base model in 4-bit mode using bitsandbytes for memory efficiency\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,      # requires bitsandbytes; lowers memory usage\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Additional model configurations\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    # Load test dataset and select only 10 examples\n",
    "    full_test_dataset = load_from_disk(\"E:\\\\fine_tuning\\\\dataset_code\")[\"test\"]\n",
    "    # test_dataset = full_test_dataset.select(range(10))\n",
    "    test_dataset = full_test_dataset\n",
    "    # Define system prompt\n",
    "    system_prompt = '''\n",
    "        **Code Generation Request** \n",
    "\n",
    "        * Read the provided **Method Descriptions & Summaries**\n",
    "        * **Complete the Body of Each Block code** based on the corresponding summaries\n",
    "        * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\n",
    "        * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\n",
    "    '''\n",
    "\n",
    "    # Preprocess function to combine the system prompt and user prompt into a single input string\n",
    "    def preprocess_example(example):\n",
    "        example[\"text\"] = (\n",
    "            \"<|im_start|>system\\n\" + system_prompt + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\" + example[\"prompt\"].strip() + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        return example\n",
    "\n",
    "    # Preprocess test dataset\n",
    "    processed_test_dataset = test_dataset.map(preprocess_example)\n",
    "\n",
    "    # Tokenization function (using truncation to 2048 tokens for compatibility with fine-tuning)\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=2048, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_test_dataset = processed_test_dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=test_dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Generate predictions using max_new_tokens instead of max_length\n",
    "    def generate_code(example):\n",
    "        input_ids = tokenizer(example[\"text\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,  # Generate 512 new tokens after the prompt\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        example[\"generated_code\"] = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return example\n",
    "\n",
    "    generated_results = tokenized_test_dataset.map(generate_code)\n",
    "\n",
    "    # Evaluate with CodeBLEU on these 10 examples\n",
    "    references = [ex[\"response\"] for ex in test_dataset]\n",
    "    hypotheses = [ex[\"generated_code\"] for ex in generated_results]\n",
    "    # Compute CodeBLEU score\n",
    "    codebleu_score = calc_codebleu(\n",
    "        references,        # list of reference code (or list of lists if there are multiple references)\n",
    "        hypotheses,        # list of candidate code\n",
    "        lang=\"c_sharp\",           # specify the programming language\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),  # weights for n-gram, weighted n-gram, syntax, and data-flow matches\n",
    "        tokenizer=None           # if None, the default string split is used\n",
    "    )\n",
    "\n",
    "    print(\"CodeBLEU Score for all examples:\", codebleu_score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perplaxity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find MSVC.\n",
      "WARNING: Failed to find Windows SDK.\n",
      "WARNING: Failed to find CUDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c927e0dbca794208bfa2646d341035ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84516a417c041b6a52b5d57b3bae87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba969062bec64efda3da6102d53dbe59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7b211d846b437ea0cd0034d6cbc7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25165621c2724d92ac9ea73ef3f13873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/7692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da196d88b1af4818be3bd7f570a5179f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51474f588f3c4157a3f76a64f44ff7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/746 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7692\n",
      "Validation dataset size: 647\n",
      "A sample validation example (tokenized):\n",
      "{'input_ids': [8948, 271, 334, 2078, 23470, 6145, 56177, 353, 4457, 279, 3897, 3070, 3523, 3874, 24685, 609, 8116, 89333, 1019, 353, 3070, 12548, 279, 13958, 315, 8886, 8362, 2038, 334, 3118, 389, 279, 12159, 68922, 198, 353, 3070, 4061, 75938, 66963, 42187, 678, 7907, 3070, 2078, 45678, 334, 304, 23725, 1182, 35078, 320, 13874, 32881, 369, 23922, 91494, 198, 353, 3070, 38121, 7036, 66963, 39565, 1172, 279, 8145, 3070, 2078, 45678, 334, 2041, 92466, 6042, 476, 1467, 271, 872, 198, 2575, 366, 2231, 397, 2575, 4895, 6563, 1321, 11603, 481, 1096, 4473, 374, 8480, 369, 11589, 29679, 6813, 304, 279, 3482, 5333, 11, 11689, 369, 1196, 16653, 323, 23715, 624, 2575, 690, 2231, 397, 2575, 366, 4684, 397, 2575, 547, 2668, 24684, 481, 1096, 536, 28872, 279, 16653, 1882, 369, 19393, 10130, 7388, 11, 22573, 429, 3847, 525, 18630, 1573, 31788, 2617, 4963, 624, 2575, 690, 4684, 1339, 2231, 4895, 6563, 1321, 11603, 1476, 262, 536, 547, 2668, 24684, 1476, 286, 1048, 366, 1708, 397, 286, 1048, 31882, 264, 501, 2867, 315, 279, 547, 2668, 24684, 536, 624, 286, 1048, 690, 1708, 397, 286, 1048, 366, 903, 829, 428, 3600, 881, 286, 1048, 1790, 320, 1900, 9381, 1648, 576, 1790, 29679, 3692, 304, 279, 1681, 15301, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 903, 829, 428, 1676, 881, 286, 1048, 2193, 320, 50, 704, 1474, 5255, 1676, 1648, 12221, 5003, 369, 279, 328, 704, 1474, 2867, 624, 286, 1048, 690, 903, 397, 286, 584, 547, 2668, 24684, 12951, 9381, 1790, 11, 328, 704, 1474, 5255, 1676, 2193, 8, 715, 286, 341, 310, 442, 14822, 220, 16, 25, 9129, 279, 1790, 29679, 3692, 304, 279, 716, 3600, 2070, 624, 310, 442, 14822, 220, 17, 25, 4230, 264, 501, 2867, 315, 328, 704, 1474, 1667, 279, 3897, 6546, 624, 286, 555, 286, 1048, 366, 1708, 397, 286, 1048, 62303, 458, 19393, 10130, 1681, 323, 12341, 369, 1196, 23715, 624, 286, 1048, 690, 1708, 397, 286, 1048, 366, 903, 829, 428, 1254, 1972, 881, 286, 1048, 1758, 1972, 320, 89611, 1648, 576, 2266, 315, 279, 1482, 10130, 1681, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 4216, 397, 286, 1048, 5430, 25, 38893, 279, 39007, 5666, 315, 8692, 279, 1681, 624, 286, 1048, 690, 4216, 397, 286, 584, 3312, 5430, 39667, 89093, 1758, 1972, 8, 715, 286, 341, 310, 442, 14822, 220, 16, 25, 7143, 279, 6885, 551, 1714, 315, 279, 547, 2668, 2867, 311, 1779, 421, 279, 1196, 374, 18630, 624, 310, 442, 14822, 220, 17, 25, 1416, 279, 1196, 374, 537, 18630, 11, 738, 279, 2033, 2639, 2038, 311, 220, 19, 15, 16, 63102, 624, 310, 442, 14822, 220, 18, 25, 96195, 11, 6423, 279, 1196, 311, 279, 5858, 2150, 320, 6182, 291, 700, 304, 279, 2038, 4292, 310, 442, 14822, 220, 19, 25, 1416, 18630, 11, 1618, 279, 1790, 29679, 3692, 304, 279, 15301, 624, 286, 555, 286, 1048, 366, 13193, 397, 286, 1048, 481, 716, 3600, 320, 1900, 9381, 1648, 65984, 279, 5785, 311, 279, 1790, 29679, 304, 279, 15301, 624, 286, 1048, 481, 547, 2668, 320, 50, 704, 1474, 1648, 19283, 315, 328, 704, 1474, 1483, 369, 11589, 1196, 16653, 624, 286, 1048, 690, 13193, 397, 262, 456, 532, 77091, 198, 73594, 66, 52917, 198, 970, 9518, 27000, 280, 970, 5100, 16757, 15641, 280, 970, 5100, 16757, 9524, 280, 970, 5100, 20526, 22179, 280, 970, 739, 7501, 8326, 280, 970, 547, 2668, 401, 2231, 4895, 6563, 1321, 11603, 198, 515, 262, 442, 1446, 1231, 1184, 311, 4582, 279, 5100, 16757, 9524, 93970, 6328, 1119, 697, 2390, 198, 262, 584, 536, 547, 2668, 24684, 198, 262, 341, 286, 869, 5762, 6145, 9381, 716, 3600, 401, 286, 869, 5762, 328, 704, 1474, 547, 2668, 280, 286, 584, 547, 2668, 24684, 12951, 9381, 1790, 11, 328, 704, 1474, 5255, 1676, 2193, 340, 286, 341, 310, 716, 3600, 284, 1790, 280, 310, 547, 2668, 284, 501, 328, 704, 1474, 8754, 317, 286, 456, 286, 584, 3312, 5430, 39667, 89093, 1758, 1972, 340, 286, 341, 310, 421, 1505, 52, 2668, 33858, 551, 19886, 1972, 1171, 310, 341, 394, 442, 1758, 1972, 12574, 38869, 12317, 2668, 22019, 6231, 17725, 19886, 1972, 9659, 29840, 6167, 11, 1758, 1972, 9659, 17474, 701, 830, 317, 394, 1758, 1972, 12574, 37828, 284, 8104, 20871, 10538, 19, 15, 16, 51181, 280, 394, 470, 280, 310, 16968, 310, 2535, 716, 3600, 19886, 1972, 8132, 286, 456, 262, 4455, 630, 13874, 3989, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "./Qwen_finetuned_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3846' max='3846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3846/3846 27:04:45, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>0.353531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.216700</td>\n",
       "      <td>0.324561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>0.311933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='647' max='647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [647/647 34:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': 0.3071354627609253, 'eval_runtime': 2082.6593, 'eval_samples_per_second': 0.311, 'eval_steps_per_second': 0.311, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk, Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def explode_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Explodes the \"text\" list in each example so that each element becomes its own example.\n",
    "    \"\"\"\n",
    "    new_examples = []\n",
    "    for example in dataset:\n",
    "        # example[\"text\"] is expected to be a list\n",
    "        for t in example[\"text\"]:\n",
    "            new_examples.append({\"text\": t})\n",
    "    return Dataset.from_list(new_examples)\n",
    "def main():\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Qwen2.5-Coder-7B-Instruct\\\\Qwen_finetuned_v4-merged\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    raw_datasets = load_from_disk('E:\\\\fine_tuning\\\\dataset_code_bank')\n",
    "\n",
    "    system_prompt = (\n",
    "        \"\\n**Code Generation Request**\\n\\n\"\n",
    "        \" * Read the provided **Method Descriptions & Summaries**\\n\"\n",
    "        \" * **Complete the Body of Each Block code** based on the corresponding summaries\\n\"\n",
    "        \" * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\\n\"\n",
    "        \" * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\\n\"\n",
    "    )\n",
    "\n",
    "    def chunk_text(text, tokenizer, max_length=1024, stride=512):\n",
    "        \"\"\"\n",
    "        Tokenize the full text and then split it into overlapping chunks.\n",
    "        Each chunk is decoded back to a string.\n",
    "        \"\"\"\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), stride):\n",
    "            chunk_tokens = tokens[i: i + max_length]\n",
    "            chunk_str = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            chunks.append(chunk_str)\n",
    "        return chunks\n",
    "\n",
    "    def preprocess_example(batch):\n",
    "        \"\"\"\n",
    "        Process a batch of examples by creating sliding window chunks.\n",
    "        Each example's text is transformed into a list of chunk strings.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for prompt, response in zip(batch[\"prompt\"], batch[\"response\"]):\n",
    "            if isinstance(prompt, list):\n",
    "                prompt = \" \".join(prompt)\n",
    "            if isinstance(response, list):\n",
    "                response = \" \".join(response)\n",
    "            \n",
    "            full_text = (\n",
    "            \"<|im_start|>system\\n\" + system_prompt + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\" + prompt.strip() + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\" + response.strip() + \"<|im_end|>\\n\"\n",
    "        )\n",
    "            \n",
    "            chunks = chunk_text(full_text, tokenizer, max_length=1024, stride=512)\n",
    "            all_chunks.append(chunks)\n",
    "        \n",
    "        return {\"text\": all_chunks}\n",
    "\n",
    "    # Process examples in batches.\n",
    "    processed_datasets = raw_datasets.map(preprocess_example, batched=True, num_proc=4)\n",
    "\n",
    "    # Explode the \"text\" list into individual examples for each split.\n",
    "    for split in processed_datasets.keys():\n",
    "        processed_datasets[split] = explode_dataset(processed_datasets[split])\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "    # Remove unnecessary columns.\n",
    "    remove_columns = processed_datasets[\"train\"].column_names\n",
    "    tokenized_datasets = processed_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "    print(\"Train dataset size:\", len(tokenized_datasets[\"train\"]))\n",
    "    print(\"Validation dataset size:\", len(tokenized_datasets[\"validation\"]))\n",
    "    if len(tokenized_datasets[\"validation\"]) > 0:\n",
    "        print(\"A sample validation example (tokenized):\")\n",
    "        print(tokenized_datasets[\"validation\"][0])\n",
    "    else:\n",
    "        print(\"Validation dataset is empty!\")\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "    save_dir = \"./Qwen_finetuned_v5\"\n",
    "    \n",
    "    print(save_dir)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_first_step=True,\n",
    "        eval_steps=1000,\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,  # Lower the eval batch size\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(\"Evaluation metrics:\", eval_metrics)\n",
    "\n",
    "    model.save_pretrained(save_dir+\"_lora\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging LoRA adapters with base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93a751b01d345fa92a2ba4823fc5567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cd02913d614f738956f9bed49b9eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to./Qwen_finetuned_v5-merged\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import  PeftModel  # Added PeftModel import\n",
    "\n",
    "def main():\n",
    "    # [Previous code remains identical until after model.save_pretrained()...]\n",
    "\n",
    "    save_dir =  \"./Qwen_finetuned_v5\"\n",
    "\n",
    "    # New section: Merge LoRA with base model\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nMerging LoRA adapters with base model...\")\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Qwen2.5-Coder-7B-Instruct\\\\Qwen_finetuned_v4-merged\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    # Reload base model in FP16 (without 4-bit quantization)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter weights\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        save_dir+\"_lora\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Merge and save\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(save_dir+\"-merged\")\n",
    "    tokenizer.save_pretrained(save_dir+\"-merged\")\n",
    "    \n",
    "    print(\"Merged model saved to\"+ save_dir +\"-merged\")\n",
    "    # --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5813730c9224510bc54bdbdda9a1d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5270b5e91a4ffbb84750c91a30970c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d74cd80d59a4188a57c52cbc34f3998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeBLEU Score for all examples: {'codebleu': 0.08708562792715882, 'ngram_match_score': 0.050650360881428415, 'weighted_ngram_match_score': 0.0889081122479649, 'syntax_match_score': 0.16321260622060885, 'dataflow_match_score': 0.045571432358633125}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "def main():\n",
    "    # Specify the model path from Hugging Face\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Qwen2.5-Coder-7B-Instruct\\\\Qwen_finetuned_v5-merged\"\n",
    "\n",
    "    # Load the tokenizer and set the padding token (using eos_token if not already set)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Adjust the tokenizer padding side to \"right\"\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Load the base model in 4-bit mode using bitsandbytes for memory efficiency\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,      # requires bitsandbytes; lowers memory usage\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Additional model configurations\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    # Load test dataset and select only 10 examples\n",
    "    full_test_dataset = load_from_disk(\"E:\\\\fine_tuning\\\\dataset_code_bank\")[\"test\"]\n",
    "    # test_dataset = full_test_dataset.select(range(10))\n",
    "    test_dataset = full_test_dataset\n",
    "    # Define system prompt\n",
    "    system_prompt = '''\n",
    "        **Code Generation Request** \n",
    "\n",
    "        * Read the provided **Method Descriptions & Summaries**\n",
    "        * **Complete the Body of Each Block code** based on the corresponding summaries\n",
    "        * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\n",
    "        * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\n",
    "    '''\n",
    "\n",
    "    # Preprocess function to combine the system prompt and user prompt into a single input string\n",
    "    def preprocess_example(example):\n",
    "        example[\"text\"] = (\n",
    "            \"<|im_start|>system\\n\" + system_prompt + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\" + example[\"prompt\"].strip() + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        return example\n",
    "\n",
    "    # Preprocess test dataset\n",
    "    processed_test_dataset = test_dataset.map(preprocess_example)\n",
    "\n",
    "    # Tokenization function (using truncation to 2048 tokens for compatibility with fine-tuning)\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=2048, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_test_dataset = processed_test_dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=test_dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Generate predictions using max_new_tokens instead of max_length\n",
    "    def generate_code(example):\n",
    "        input_ids = tokenizer(example[\"text\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,  # Generate 512 new tokens after the prompt\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        example[\"generated_code\"] = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return example\n",
    "\n",
    "    generated_results = tokenized_test_dataset.map(generate_code)\n",
    "\n",
    "    # Evaluate with CodeBLEU on these 10 examples\n",
    "    references = [ex[\"response\"] for ex in test_dataset]\n",
    "    hypotheses = [ex[\"generated_code\"] for ex in generated_results]\n",
    "    # Compute CodeBLEU score\n",
    "    codebleu_score = calc_codebleu(\n",
    "        references,        # list of reference code (or list of lists if there are multiple references)\n",
    "        hypotheses,        # list of candidate code\n",
    "        lang=\"c_sharp\",           # specify the programming language\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),  # weights for n-gram, weighted n-gram, syntax, and data-flow matches\n",
    "        tokenizer=None           # if None, the default string split is used\n",
    "    )\n",
    "\n",
    "    print(\"CodeBLEU Score for all examples:\", codebleu_score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reapeted train on bank code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find MSVC.\n",
      "WARNING: Failed to find Windows SDK.\n",
      "WARNING: Failed to find CUDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f82415c8fe4302b94e535b1236696b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22159f970e74ec7b1fc37f3aa842592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/7692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa19e6f0be3348f9a939257fc02599f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3d94bcc05e47009f410fc8fc444c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/746 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 7692\n",
      "Validation dataset size: 647\n",
      "A sample validation example (tokenized):\n",
      "{'input_ids': [8948, 271, 334, 2078, 23470, 6145, 56177, 353, 4457, 279, 3897, 3070, 3523, 3874, 24685, 609, 8116, 89333, 1019, 353, 3070, 12548, 279, 13958, 315, 8886, 8362, 2038, 334, 3118, 389, 279, 12159, 68922, 198, 353, 3070, 4061, 75938, 66963, 42187, 678, 7907, 3070, 2078, 45678, 334, 304, 23725, 1182, 35078, 320, 13874, 32881, 369, 23922, 91494, 198, 353, 3070, 38121, 7036, 66963, 39565, 1172, 279, 8145, 3070, 2078, 45678, 334, 2041, 92466, 6042, 476, 1467, 271, 872, 198, 2575, 366, 2231, 397, 2575, 4895, 6563, 1321, 11603, 481, 1096, 4473, 374, 8480, 369, 11589, 29679, 6813, 304, 279, 3482, 5333, 11, 11689, 369, 1196, 16653, 323, 23715, 624, 2575, 690, 2231, 397, 2575, 366, 4684, 397, 2575, 547, 2668, 24684, 481, 1096, 536, 28872, 279, 16653, 1882, 369, 19393, 10130, 7388, 11, 22573, 429, 3847, 525, 18630, 1573, 31788, 2617, 4963, 624, 2575, 690, 4684, 1339, 2231, 4895, 6563, 1321, 11603, 1476, 262, 536, 547, 2668, 24684, 1476, 286, 1048, 366, 1708, 397, 286, 1048, 31882, 264, 501, 2867, 315, 279, 547, 2668, 24684, 536, 624, 286, 1048, 690, 1708, 397, 286, 1048, 366, 903, 829, 428, 3600, 881, 286, 1048, 1790, 320, 1900, 9381, 1648, 576, 1790, 29679, 3692, 304, 279, 1681, 15301, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 903, 829, 428, 1676, 881, 286, 1048, 2193, 320, 50, 704, 1474, 5255, 1676, 1648, 12221, 5003, 369, 279, 328, 704, 1474, 2867, 624, 286, 1048, 690, 903, 397, 286, 584, 547, 2668, 24684, 12951, 9381, 1790, 11, 328, 704, 1474, 5255, 1676, 2193, 8, 715, 286, 341, 310, 442, 14822, 220, 16, 25, 9129, 279, 1790, 29679, 3692, 304, 279, 716, 3600, 2070, 624, 310, 442, 14822, 220, 17, 25, 4230, 264, 501, 2867, 315, 328, 704, 1474, 1667, 279, 3897, 6546, 624, 286, 555, 286, 1048, 366, 1708, 397, 286, 1048, 62303, 458, 19393, 10130, 1681, 323, 12341, 369, 1196, 23715, 624, 286, 1048, 690, 1708, 397, 286, 1048, 366, 903, 829, 428, 1254, 1972, 881, 286, 1048, 1758, 1972, 320, 89611, 1648, 576, 2266, 315, 279, 1482, 10130, 1681, 624, 286, 1048, 690, 903, 397, 286, 1048, 366, 4216, 397, 286, 1048, 5430, 25, 38893, 279, 39007, 5666, 315, 8692, 279, 1681, 624, 286, 1048, 690, 4216, 397, 286, 584, 3312, 5430, 39667, 89093, 1758, 1972, 8, 715, 286, 341, 310, 442, 14822, 220, 16, 25, 7143, 279, 6885, 551, 1714, 315, 279, 547, 2668, 2867, 311, 1779, 421, 279, 1196, 374, 18630, 624, 310, 442, 14822, 220, 17, 25, 1416, 279, 1196, 374, 537, 18630, 11, 738, 279, 2033, 2639, 2038, 311, 220, 19, 15, 16, 63102, 624, 310, 442, 14822, 220, 18, 25, 96195, 11, 6423, 279, 1196, 311, 279, 5858, 2150, 320, 6182, 291, 700, 304, 279, 2038, 4292, 310, 442, 14822, 220, 19, 25, 1416, 18630, 11, 1618, 279, 1790, 29679, 3692, 304, 279, 15301, 624, 286, 555, 286, 1048, 366, 13193, 397, 286, 1048, 481, 716, 3600, 320, 1900, 9381, 1648, 65984, 279, 5785, 311, 279, 1790, 29679, 304, 279, 15301, 624, 286, 1048, 481, 547, 2668, 320, 50, 704, 1474, 1648, 19283, 315, 328, 704, 1474, 1483, 369, 11589, 1196, 16653, 624, 286, 1048, 690, 13193, 397, 262, 456, 532, 77091, 198, 73594, 66, 52917, 198, 970, 9518, 27000, 280, 970, 5100, 16757, 15641, 280, 970, 5100, 16757, 9524, 280, 970, 5100, 20526, 22179, 280, 970, 739, 7501, 8326, 280, 970, 547, 2668, 401, 2231, 4895, 6563, 1321, 11603, 198, 515, 262, 442, 1446, 1231, 1184, 311, 4582, 279, 5100, 16757, 9524, 93970, 6328, 1119, 697, 2390, 198, 262, 584, 536, 547, 2668, 24684, 198, 262, 341, 286, 869, 5762, 6145, 9381, 716, 3600, 401, 286, 869, 5762, 328, 704, 1474, 547, 2668, 280, 286, 584, 547, 2668, 24684, 12951, 9381, 1790, 11, 328, 704, 1474, 5255, 1676, 2193, 340, 286, 341, 310, 716, 3600, 284, 1790, 280, 310, 547, 2668, 284, 501, 328, 704, 1474, 8754, 317, 286, 456, 286, 584, 3312, 5430, 39667, 89093, 1758, 1972, 340, 286, 341, 310, 421, 1505, 52, 2668, 33858, 551, 19886, 1972, 1171, 310, 341, 394, 442, 1758, 1972, 12574, 38869, 12317, 2668, 22019, 6231, 17725, 19886, 1972, 9659, 29840, 6167, 11, 1758, 1972, 9659, 17474, 701, 830, 317, 394, 1758, 1972, 12574, 37828, 284, 8104, 20871, 10538, 19, 15, 16, 51181, 280, 394, 470, 280, 310, 16968, 310, 2535, 716, 3600, 19886, 1972, 8132, 286, 456, 262, 4455, 630, 13874, 3989, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "./Qwen_finetuned_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3846' max='3846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3846/3846 27:08:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.357036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>0.326470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.313923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='647' max='647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [647/647 34:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': 0.3091481029987335, 'eval_runtime': 2082.384, 'eval_samples_per_second': 0.311, 'eval_steps_per_second': 0.311, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk, Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def explode_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Explodes the \"text\" list in each example so that each element becomes its own example.\n",
    "    \"\"\"\n",
    "    new_examples = []\n",
    "    for example in dataset:\n",
    "        # example[\"text\"] is expected to be a list\n",
    "        for t in example[\"text\"]:\n",
    "            new_examples.append({\"text\": t})\n",
    "    return Dataset.from_list(new_examples)\n",
    "def main():\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Qwen2.5-Coder-7B-Instruct\\\\Qwen_finetuned_v4-merged\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    raw_datasets = load_from_disk('E:\\\\fine_tuning\\\\dataset_code_bank')\n",
    "\n",
    "    system_prompt = (\n",
    "        \"\\n**Code Generation Request**\\n\\n\"\n",
    "        \" * Read the provided **Method Descriptions & Summaries**\\n\"\n",
    "        \" * **Complete the Body of Each Block code** based on the corresponding summaries\\n\"\n",
    "        \" * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\\n\"\n",
    "        \" * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\\n\"\n",
    "    )\n",
    "\n",
    "    def chunk_text(text, tokenizer, max_length=1024, stride=512):\n",
    "        \"\"\"\n",
    "        Tokenize the full text and then split it into overlapping chunks.\n",
    "        Each chunk is decoded back to a string.\n",
    "        \"\"\"\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), stride):\n",
    "            chunk_tokens = tokens[i: i + max_length]\n",
    "            chunk_str = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            chunks.append(chunk_str)\n",
    "        return chunks\n",
    "\n",
    "    def preprocess_example(batch):\n",
    "        \"\"\"\n",
    "        Process a batch of examples by creating sliding window chunks.\n",
    "        Each example's text is transformed into a list of chunk strings.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for prompt, response in zip(batch[\"prompt\"], batch[\"response\"]):\n",
    "            if isinstance(prompt, list):\n",
    "                prompt = \" \".join(prompt)\n",
    "            if isinstance(response, list):\n",
    "                response = \" \".join(response)\n",
    "            \n",
    "            full_text = (\n",
    "            \"<|im_start|>system\\n\" + system_prompt + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\" + prompt.strip() + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\" + response.strip() + \"<|im_end|>\\n\"\n",
    "        )\n",
    "            \n",
    "            chunks = chunk_text(full_text, tokenizer, max_length=1024, stride=512)\n",
    "            all_chunks.append(chunks)\n",
    "        \n",
    "        return {\"text\": all_chunks}\n",
    "\n",
    "    # Process examples in batches.\n",
    "    processed_datasets = raw_datasets.map(preprocess_example, batched=True, num_proc=4)\n",
    "\n",
    "    # Explode the \"text\" list into individual examples for each split.\n",
    "    for split in processed_datasets.keys():\n",
    "        processed_datasets[split] = explode_dataset(processed_datasets[split])\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "    # Remove unnecessary columns.\n",
    "    remove_columns = processed_datasets[\"train\"].column_names\n",
    "    tokenized_datasets = processed_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "    print(\"Train dataset size:\", len(tokenized_datasets[\"train\"]))\n",
    "    print(\"Validation dataset size:\", len(tokenized_datasets[\"validation\"]))\n",
    "    if len(tokenized_datasets[\"validation\"]) > 0:\n",
    "        print(\"A sample validation example (tokenized):\")\n",
    "        print(tokenized_datasets[\"validation\"][0])\n",
    "    else:\n",
    "        print(\"Validation dataset is empty!\")\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "    save_dir = \"./Qwen_finetuned_v5\"\n",
    "    \n",
    "    print(save_dir)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_first_step=True,\n",
    "        eval_steps=1000,\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,  # Lower the eval batch size\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(\"Evaluation metrics:\", eval_metrics)\n",
    "\n",
    "    model.save_pretrained(save_dir+\"_lora\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging LoRA adapters with base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\fine_tuning\\env\\lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f48e0c0ed4949f294191a316d6ea49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\fine_tuning\\env\\lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to./Qwen_finetuned_v5-merged\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import  PeftModel  # Added PeftModel import\n",
    "\n",
    "def main():\n",
    "    # [Previous code remains identical until after model.save_pretrained()...]\n",
    "\n",
    "    save_dir =  \"./Qwen_finetuned_v5\"\n",
    "\n",
    "    # New section: Merge LoRA with base model\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nMerging LoRA adapters with base model...\")\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Qwen2.5-Coder-7B-Instruct\\\\Qwen_finetuned_v4-merged\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    # Reload base model in FP16 (without 4-bit quantization)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter weights\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        save_dir+\"_lora\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Merge and save\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(save_dir+\"-merged\")\n",
    "    tokenizer.save_pretrained(save_dir+\"-merged\")\n",
    "    \n",
    "    print(\"Merged model saved to\"+ save_dir +\"-merged\")\n",
    "    # --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find MSVC.\n",
      "WARNING: Failed to find Windows SDK.\n",
      "WARNING: Failed to find CUDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c7e70388314552b784de7a18c962ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967cf146745146d997e3b71082fa9d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeBLEU Score for all examples: {'codebleu': 0.28420149107786136, 'ngram_match_score': 0.09453043295781977, 'weighted_ngram_match_score': 0.21753594188659264, 'syntax_match_score': 0.6813711739955449, 'dataflow_match_score': 0.14336841547148804}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "def main():\n",
    "    # Specify the model path from Hugging Face\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Qwen2.5-Coder-7B-Instruct\\\\Qwen_finetuned_v5-merged\"\n",
    "\n",
    "    # Load the tokenizer and set the padding token (using eos_token if not already set)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Adjust the tokenizer padding side to \"right\"\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Load the base model in 4-bit mode using bitsandbytes for memory efficiency\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,      # requires bitsandbytes; lowers memory usage\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Additional model configurations\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    # Load test dataset and select only 10 examples\n",
    "    full_test_dataset = load_from_disk(\"E:\\\\fine_tuning\\\\dataset_code_bank\")[\"test\"]\n",
    "    # test_dataset = full_test_dataset.select(range(10))\n",
    "    test_dataset = full_test_dataset\n",
    "    # Define system prompt\n",
    "    system_prompt = '''\n",
    "        **Code Generation Request** \n",
    "\n",
    "        * Read the provided **Method Descriptions & Summaries**\n",
    "        * **Complete the Body of Each Block code** based on the corresponding summaries\n",
    "        * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\n",
    "        * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\n",
    "    '''\n",
    "\n",
    "    # Preprocess function to combine the system prompt and user prompt into a single input string\n",
    "    def preprocess_example(example):\n",
    "        example[\"text\"] = (\n",
    "            \"<|im_start|>system\\n\" + system_prompt + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\" + example[\"prompt\"].strip() + \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        return example\n",
    "\n",
    "    # Preprocess test dataset\n",
    "    processed_test_dataset = test_dataset.map(preprocess_example)\n",
    "\n",
    "    # Tokenization function (using truncation to 2048 tokens for compatibility with fine-tuning)\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=2048, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_test_dataset = processed_test_dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=test_dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Generate predictions using max_new_tokens instead of max_length\n",
    "    def generate_code(example):\n",
    "        input_ids = tokenizer(example[\"text\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,  # Generate 512 new tokens after the prompt\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        example[\"generated_code\"] = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return example\n",
    "\n",
    "    generated_results = tokenized_test_dataset.map(generate_code)\n",
    "\n",
    "    # Evaluate with CodeBLEU on these 10 examples\n",
    "    references = [ex[\"response\"] for ex in test_dataset]\n",
    "    hypotheses = [ex[\"generated_code\"] for ex in generated_results]\n",
    "    # Compute CodeBLEU score\n",
    "    codebleu_score = calc_codebleu(\n",
    "        references,        # list of reference code (or list of lists if there are multiple references)\n",
    "        hypotheses,        # list of candidate code\n",
    "        lang=\"c_sharp\",           # specify the programming language\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),  # weights for n-gram, weighted n-gram, syntax, and data-flow matches\n",
    "        tokenizer=None           # if None, the default string split is used\n",
    "    )\n",
    "\n",
    "    print(\"CodeBLEU Score for all examples:\", codebleu_score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
