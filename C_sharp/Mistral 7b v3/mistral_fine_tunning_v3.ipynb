{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find MSVC.\n",
      "WARNING: Failed to find Windows SDK.\n",
      "WARNING: Failed to find CUDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d598511ac3944b92a6f572ffd2be6244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8802aef20445c1b15f1f89e2ef74e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/9641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327aef54bc4b45e8a1820b6988e72e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2488 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2841c934d008428f883d57dd8f456177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 9641\n",
      "Validation dataset size: 2488\n",
      "A sample validation example (tokenized):\n",
      "{'input_ids': [1, 2903, 29515, 29473, 781, 1116, 3308, 27570, 12284, 1116, 781, 781, 1166, 5707, 1040, 4625, 1387, 4710, 3303, 17903, 1335, 7695, 4247, 1265, 1116, 781, 1166, 1387, 15696, 1040, 17018, 1070, 7834, 9831, 3464, 1116, 3586, 1124, 1040, 8374, 18830, 1265, 781, 1166, 1387, 5824, 8019, 11779, 4817, 1162, 2010, 1312, 7906, 1387, 3308, 9831, 29481, 1116, 1065, 22980, 1620, 29475, 6214, 1093, 14708, 26688, 1122, 22501, 1988, 3205, 781, 1166, 1387, 10257, 7966, 6864, 4817, 7901, 1315, 1633, 1040, 8136, 1387, 3308, 9831, 29481, 1116, 2439, 11696, 6145, 8384, 1210, 3013, 781, 781, 1498, 29515, 2722, 1291, 14915, 29535, 781, 4080, 8670, 29491, 29523, 1349, 29478, 29491, 8055, 1056, 1155, 1619, 5540, 1117, 8100, 1122, 8269, 19507, 1066, 9362, 1789, 9299, 1065, 1040, 28572, 29478, 5761, 11550, 29491, 781, 4080, 2635, 14915, 29535, 781, 4080, 1291, 7286, 29535, 781, 4080, 28572, 29478, 3409, 9299, 8355, 20672, 1155, 1619, 1643, 6080, 8991, 6330, 1122, 9601, 1072, 16865, 2972, 1789, 9299, 1065, 1040, 28572, 29478, 5761, 29491, 781, 4080, 2635, 7286, 29535, 781, 781, 14915, 8670, 29491, 29523, 1349, 29478, 29491, 8055, 1056, 1139, 781, 781, 3055, 1830, 11241, 1643, 28572, 29478, 3409, 9299, 8355, 20672, 1139, 781, 781, 6158, 2722, 1291, 4267, 29535, 781, 6158, 2722, 4069, 29481, 5934, 1789, 9299, 1066, 1040, 1789, 9299, 6210, 29491, 781, 6158, 2722, 2635, 4267, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 3393, 9299, 8355, 2123, 781, 6158, 2722, 1789, 9299, 8355, 1093, 4787, 1349, 29478, 3409, 9299, 8355, 2097, 1183, 6210, 1066, 1458, 1789, 9299, 1390, 1115, 4654, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 3393, 9299, 2123, 781, 6158, 2722, 1789, 9299, 1093, 18566, 29557, 1773, 29493, 6475, 29535, 2097, 1098, 20211, 13920, 2972, 5282, 1066, 1420, 8374, 10462, 5282, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 2722, 1291, 8385, 29535, 781, 6158, 2722, 18361, 1349, 29478, 3409, 9299, 8355, 29515, 7467, 1040, 9225, 1789, 9299, 6210, 29491, 781, 6158, 2722, 2635, 8385, 29535, 781, 6158, 1566, 1830, 18361, 1349, 29478, 3409, 9299, 8355, 4069, 3409, 9299, 29500, 4787, 1349, 29478, 3409, 9299, 8355, 1789, 9299, 8355, 29493, 23596, 29557, 1773, 29493, 6475, 29535, 1789, 9299, 29499, 29473, 781, 6158, 1139, 781, 18190, 1357, 8036, 29473, 29508, 29515, 21204, 1148, 1827, 2198, 2713, 29501, 2199, 6732, 1065, 1040, 1789, 9299, 20211, 29491, 781, 18190, 1357, 8036, 29473, 29518, 29515, 2031, 2198, 10462, 29493, 1735, 1146, 1158, 1032, 1971, 1490, 3140, 1066, 1040, 1789, 9299, 6210, 29491, 781, 18190, 1357, 10990, 8036, 29515, 5339, 1040, 9225, 1789, 9299, 6210, 29491, 781, 6158, 1211, 781, 781, 6158, 2722, 1291, 4267, 29535, 781, 6158, 2722, 4069, 29481, 1032, 3460, 10462, 1066, 1040, 1789, 9299, 6210, 29491, 781, 6158, 2722, 2635, 4267, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 3393, 9299, 8355, 2123, 781, 6158, 2722, 1789, 9299, 8355, 1093, 4787, 1349, 29478, 3409, 9299, 8355, 2097, 1183, 6210, 1066, 1458, 1040, 10462, 1390, 1115, 4654, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 2346, 1773, 2123, 781, 6158, 2722, 2972, 1773, 1093, 1773, 2097, 1183, 1980, 1070, 1040, 2972, 1122, 1458, 1040, 10462, 1117, 2018, 4654, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 9530, 1773, 2123, 781, 6158, 2722, 10462, 1773, 1093, 1773, 2097, 1183, 1980, 1070, 1040, 10462, 1066, 1115, 6131, 1163, 1040, 2972, 1980, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 2722, 1291, 8385, 29535, 781, 6158, 2722, 18361, 1349, 29478, 3409, 9299, 8355, 29515, 7467, 1040, 9225, 1789, 9299, 6210, 29491, 781, 6158, 2722, 2635, 8385, 29535, 781, 6158, 1566, 1830, 18361, 1349, 29478, 3409, 9299, 8355, 4069, 4957, 29500, 4787, 1349, 29478, 3409, 9299, 8355, 1789, 9299, 8355, 29493, 6475, 2972, 1773, 29493, 6475, 10462, 1773, 29499, 29473, 781, 6158, 1139, 781, 18190, 1357, 8036, 29473, 29508, 29515, 4069, 1040, 6908, 10462, 1158, 1032, 1971, 1490, 3140, 1122, 1040, 2846, 2972, 1980, 29491, 781, 18190, 1357, 10990, 8036, 29515, 5339, 1040, 9225, 1789, 9299, 6210, 29491, 781, 6158, 1211, 781, 781, 6158, 2722, 1291, 4267, 29535, 781, 6158, 2722, 4069, 29481, 1032, 13689, 10462, 1066, 1040, 1789, 9299, 6210, 29491, 781, 6158, 2722, 2635, 4267, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 3393, 9299, 8355, 2123, 781, 6158, 2722, 1789, 9299, 8355, 1093, 4787, 1349, 29478, 3409, 9299, 8355, 2097, 1183, 6210, 1066, 1458, 1040, 10462, 1390, 1115, 4654, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 2722, 1291, 8385, 29535, 781, 6158, 2722, 18361, 1349, 29478, 3409, 9299, 8355, 29515, 7467, 1040, 9225, 1789, 9299, 6210, 29491, 781, 6158, 2722, 2635, 8385, 29535, 781, 6158, 1566, 1830, 18361, 1349, 29478, 3409, 9299, 8355, 4069, 4957, 29557, 29506, 1773, 29493, 1088, 1773, 7907, 4091, 4787, 1349, 29478, 3409, 9299, 8355, 1789, 9299, 8355, 29499, 781, 18190, 1738, 1088, 1773, 1482, 1083, 2854, 781, 18190, 1738, 1088, 1773, 7907, 1482, 1083, 2854, 4957, 781, 6158, 1139, 781, 18190, 1357, 8036, 29473, 29508, 29515, 4069, 1040, 13689, 10462, 1158, 1032, 1971, 1490, 3140, 1122, 1040, 6908, 2972, 1980, 29491, 781, 18190, 1357, 10990, 8036, 29515, 5339, 1040, 9225, 1789, 9299, 6210, 29491, 781, 6158, 1211, 781, 781, 6158, 2722, 1291, 4267, 29535, 781, 6158, 2722, 1088, 2808, 1066, 1735, 5934, 1789, 9299, 1066, 1040, 1789, 9299, 6210, 1281, 1358, 1279, 1227, 2909, 3356, 29491, 781, 6158, 2722, 2635, 4267, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 3393, 9299, 8355, 2123, 781, 6158, 2722, 1789, 9299, 8355, 1093, 4787, 1349, 29478, 3409, 9299, 8355, 2097, 1183, 6210, 1066, 1458, 1789, 9299, 1390, 1115, 4654, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 3393, 9299, 2123, 781, 6158, 2722, 1789, 9299, 1093, 18566, 29557, 1773, 29493, 6475, 29535, 2097, 1098, 20211, 13920, 2972, 5282, 1066, 1420, 8374, 10462, 5282, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 2722, 1291, 8385, 29535, 781, 6158, 2722, 18361, 1349, 29478, 3409, 9299, 8355, 29515, 7467, 1040, 9225, 1789, 9299, 6210, 29491, 781, 6158, 2722, 2635, 8385, 29535, 781, 6158, 1566, 1830, 18361, 1349, 29478, 3409, 9299, 8355, 12210, 2759, 3409, 9299, 29500, 4787, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}\n",
      "Saving output to: ./mistral_finetuned_v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7230' max='7230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7230/7230 52:19:32, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.558384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.532100</td>\n",
       "      <td>0.544873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>0.549312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.457500</td>\n",
       "      <td>0.543884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.405200</td>\n",
       "      <td>0.556317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.389100</td>\n",
       "      <td>0.557123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.386000</td>\n",
       "      <td>0.558952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3f1889a7-c2a6-4cb2-b6c9-94ec3f5d702e)') - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0d2ebc84-e49c-47c9-a01a-d75fdaf3b605)') - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 22ae93de-60d1-4fc3-9e75-005d4eb50894)') - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b7571307-bfab-421c-975e-5e55bbc92f1f)') - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8de5a624-9a0b-49c1-aead-4a5bfdcc709e)') - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: fe038b20-e51c-4901-b401-965a16ecc660)') - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 50aa381c-0250-447a-83ef-3cbfc164c5ed)') - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 28057f30-8d62-403d-a1eb-e8d9b488001c)') - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 1fcf03d4-ec0a-46b3-ab22-cd08c60cd62b)') - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='311' max='311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [311/311 1:36:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': 0.5587656497955322, 'eval_runtime': 5821.9496, 'eval_samples_per_second': 0.427, 'eval_steps_per_second': 0.053, 'epoch': 2.9990664868789545}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk, Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def explode_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Explodes the \"text\" list in each example so that each element becomes its own example.\n",
    "    \"\"\"\n",
    "    new_examples = []\n",
    "    for example in dataset:\n",
    "        # example[\"text\"] is expected to be a list\n",
    "        for t in example[\"text\"]:\n",
    "            new_examples.append({\"text\": t})\n",
    "    return Dataset.from_list(new_examples)\n",
    "\n",
    "def main():\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    raw_datasets = load_from_disk('E:\\\\fine_tuning\\\\dataset_code')\n",
    "\n",
    "    system_prompt = (\n",
    "        \"\\n**Code Generation Request**\\n\\n\"\n",
    "        \" * Read the provided **Method Descriptions & Summaries**\\n\"\n",
    "        \" * **Complete the Body of Each Block code** based on the corresponding summaries\\n\"\n",
    "        \" * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\\n\"\n",
    "        \" * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\\n\"\n",
    "    )\n",
    "\n",
    "    def chunk_text(text, tokenizer, max_length=1024, stride=512):\n",
    "        \"\"\"\n",
    "        Tokenize the full text and then split it into overlapping chunks.\n",
    "        Each chunk is decoded back to a string.\n",
    "        \"\"\"\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), stride):\n",
    "            chunk_tokens = tokens[i: i + max_length]\n",
    "            chunk_str = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            chunks.append(chunk_str)\n",
    "        return chunks\n",
    "\n",
    "    def preprocess_example(batch):\n",
    "        \"\"\"\n",
    "        Process a batch of examples by creating sliding window chunks.\n",
    "        Each example's text is transformed into a list of chunk strings.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for prompt, response in zip(batch[\"prompt\"], batch[\"response\"]):\n",
    "            if isinstance(prompt, list):\n",
    "                prompt = \" \".join(prompt)\n",
    "            if isinstance(response, list):\n",
    "                response = \" \".join(response)\n",
    "            \n",
    "            full_text = (\n",
    "                \"<s>[INST] System: \" + system_prompt + \"\\n\"\n",
    "                \"User: \" + prompt.strip() + \" [/INST] \\n\"\n",
    "                \"Assistant: \" + response.strip() + \" </s>\"\n",
    "            )\n",
    "            \n",
    "            chunks = chunk_text(full_text, tokenizer, max_length=1024, stride=512)\n",
    "            all_chunks.append(chunks)\n",
    "        \n",
    "        return {\"text\": all_chunks}\n",
    "\n",
    "    # Process examples in batches.\n",
    "    processed_datasets = raw_datasets.map(preprocess_example, batched=True, num_proc=4)\n",
    "\n",
    "    # Explode the \"text\" list into individual examples for each split.\n",
    "    for split in processed_datasets.keys():\n",
    "        processed_datasets[split] = explode_dataset(processed_datasets[split])\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "    # Remove unnecessary columns.\n",
    "    remove_columns = processed_datasets[\"train\"].column_names\n",
    "    tokenized_datasets = processed_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "    print(\"Train dataset size:\", len(tokenized_datasets[\"train\"]))\n",
    "    print(\"Validation dataset size:\", len(tokenized_datasets[\"validation\"]))\n",
    "    if len(tokenized_datasets[\"validation\"]) > 0:\n",
    "        print(\"A sample validation example (tokenized):\")\n",
    "        print(tokenized_datasets[\"validation\"][0])\n",
    "    else:\n",
    "        print(\"Validation dataset is empty!\")\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    save_dir = \"./mistral_finetuned_v4\"\n",
    "    print(\"Saving output to:\", save_dir)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_first_step=True,\n",
    "        eval_steps=1000,\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(\"Evaluation metrics:\", eval_metrics)\n",
    "\n",
    "    model.save_pretrained(save_dir + \"_lora\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging LoRA adapters with base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a353b6047d4b04982d00be0f4ee829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da99a79c42da406d90a72ae3eb5455d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to./mistral_finetuned_v4-merged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, get_peft_model, PeftModel  # Added PeftModel import\n",
    "\n",
    "def main():\n",
    "    # [Previous code remains identical until after model.save_pretrained()...]\n",
    "\n",
    "    save_dir = \"./mistral_finetuned_v4\"\n",
    "\n",
    "    # New section: Merge LoRA with base model\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nMerging LoRA adapters with base model...\")\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    # Reload base model in FP16 (without 4-bit quantization)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter weights\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        save_dir+\"_lora\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Merge and save\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(save_dir+\"-merged\")\n",
    "    tokenizer.save_pretrained(save_dir+\"-merged\")\n",
    "    \n",
    "    print(\"Merged model saved to\"+ save_dir +\"-merged\")\n",
    "    # --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find MSVC.\n",
      "WARNING: Failed to find Windows SDK.\n",
      "WARNING: Failed to find CUDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e748f030d9b34635a59d545edaf07f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6006a07eab419e86e7561de3cd6ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0427b90d3d0141e395586694645781e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/632 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeBLEU Score for all examples: {'codebleu': 0.2742492493504123, 'ngram_match_score': 0.10951106428853616, 'weighted_ngram_match_score': 0.3116537412315968, 'syntax_match_score': 0.2641481075916413, 'dataflow_match_score': 0.41168408428987496}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "def main():\n",
    "    # Specify the model path from Hugging Face\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Mistral_v3_instruct\\\\mistral_finetuned_v4-merged\"\n",
    "\n",
    "    # Load the tokenizer and set the padding token (using eos_token if not already set)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Adjust the tokenizer padding side to \"right\"\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Load the base model in 4-bit mode using bitsandbytes for memory efficiency\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,      # requires bitsandbytes; lowers memory usage\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Additional model configurations\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    # Load test dataset and select only 10 examples\n",
    "    full_test_dataset = load_from_disk(\"E:\\\\fine_tuning\\\\dataset_code\")[\"test\"]\n",
    "    # test_dataset = full_test_dataset.select(range(10))\n",
    "    test_dataset = full_test_dataset\n",
    "    # Define system prompt\n",
    "    system_prompt = '''\n",
    "        **Code Generation Request** \n",
    "\n",
    "        * Read the provided **Method Descriptions & Summaries**\n",
    "        * **Complete the Body of Each Block code** based on the corresponding summaries\n",
    "        * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\n",
    "        * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\n",
    "    '''\n",
    "\n",
    "    # Preprocess function to combine the system prompt and user prompt into a single input string\n",
    "    def preprocess_example(example):\n",
    "        example[\"text\"] = (\n",
    "            \"<s>[INST] \"\n",
    "            \"System: \" + system_prompt + \"\\n\"\n",
    "            \"User: \" + example[\"prompt\"].strip() + \" [/INST] \\n\"\n",
    "            \"Assistant: \"\n",
    "        )\n",
    "        return example\n",
    "\n",
    "    # Preprocess test dataset\n",
    "    processed_test_dataset = test_dataset.map(preprocess_example)\n",
    "\n",
    "    # Tokenization function (using truncation to 2048 tokens for compatibility with fine-tuning)\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=2048, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_test_dataset = processed_test_dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=test_dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Generate predictions using max_new_tokens instead of max_length\n",
    "    def generate_code(example):\n",
    "        input_ids = tokenizer(example[\"text\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,  # Generate 512 new tokens after the prompt\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        example[\"generated_code\"] = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return example\n",
    "\n",
    "    generated_results = tokenized_test_dataset.map(generate_code)\n",
    "\n",
    "    # Evaluate with CodeBLEU on these 10 examples\n",
    "    references = [ex[\"response\"] for ex in test_dataset]\n",
    "    hypotheses = [ex[\"generated_code\"] for ex in generated_results]\n",
    "    # Compute CodeBLEU score\n",
    "    codebleu_score = calc_codebleu(\n",
    "        references,        # list of reference code (or list of lists if there are multiple references)\n",
    "        hypotheses,        # list of candidate code\n",
    "        lang=\"c_sharp\",           # specify the programming language\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),  # weights for n-gram, weighted n-gram, syntax, and data-flow matches\n",
    "        tokenizer=None           # if None, the default string split is used\n",
    "    )\n",
    "\n",
    "    print(\"CodeBLEU Score for all examples:\", codebleu_score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perplaxity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find MSVC.\n",
      "WARNING: Failed to find Windows SDK.\n",
      "WARNING: Failed to find CUDA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dcce047ef8437eacad1e2efbff7318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238c298ef9a34bf5949b2f142de0d4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565fdc1f8992487390bbaa0d21e5ad9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd3afb2f46f4cb38a2bbe5edce1fba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 10552\n",
      "Validation dataset size: 873\n",
      "A sample validation example (tokenized):\n",
      "{'input_ids': [1, 2903, 29515, 29473, 781, 1116, 3308, 27570, 12284, 1116, 781, 781, 1166, 5707, 1040, 4625, 1387, 4710, 3303, 17903, 1335, 7695, 4247, 1265, 1116, 781, 1166, 1387, 15696, 1040, 17018, 1070, 7834, 9831, 3464, 1116, 3586, 1124, 1040, 8374, 18830, 1265, 781, 1166, 1387, 5824, 8019, 11779, 4817, 1162, 2010, 1312, 7906, 1387, 3308, 9831, 29481, 1116, 1065, 22980, 1620, 29475, 6214, 1093, 14708, 26688, 1122, 22501, 1988, 3205, 781, 1166, 1387, 10257, 7966, 6864, 4817, 7901, 1315, 1633, 1040, 8136, 1387, 3308, 9831, 29481, 1116, 2439, 11696, 6145, 8384, 1210, 3013, 781, 781, 1498, 29515, 2722, 1291, 14915, 29535, 781, 4080, 7121, 7887, 29491, 27053, 1155, 1619, 5540, 1117, 8100, 1122, 13620, 5754, 2474, 8844, 1065, 1040, 5454, 7854, 29493, 10875, 1122, 2956, 21426, 1072, 3995, 2605, 29491, 781, 4080, 2635, 14915, 29535, 781, 4080, 1291, 7286, 29535, 781, 4080, 1268, 4595, 27053, 1155, 1619, 1643, 1444, 2059, 1040, 21426, 2527, 1122, 22618, 14612, 10596, 29493, 20851, 1137, 6211, 1228, 24011, 1927, 3503, 1056, 5211, 6591, 29491, 781, 4080, 2635, 7286, 29535, 781, 781, 14915, 7121, 7887, 29491, 27053, 1139, 781, 781, 3055, 1643, 1268, 4595, 27053, 1139, 781, 781, 6158, 2722, 1291, 4267, 29535, 781, 6158, 2722, 20783, 5772, 1032, 1401, 4103, 1070, 1040, 1268, 4595, 27053, 1643, 29491, 781, 6158, 2722, 2635, 4267, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 4191, 2123, 781, 6158, 2722, 2447, 1093, 2980, 18063, 2097, 1183, 2447, 5754, 2474, 6409, 1065, 1040, 2927, 17660, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 3009, 2123, 781, 6158, 2722, 3471, 1093, 29503, 1435, 1498, 29491, 29547, 3009, 2097, 20826, 7240, 1122, 1040, 1086, 1435, 1498, 4103, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 1566, 1268, 4595, 27053, 29500, 2980, 18063, 2447, 29493, 1086, 1435, 1498, 29491, 29547, 3009, 3471, 29499, 29473, 781, 6158, 1139, 781, 18190, 1357, 8036, 29473, 29508, 29515, 13997, 1040, 2447, 5754, 2474, 6409, 1065, 1040, 1351, 4191, 2602, 29491, 781, 18190, 1357, 8036, 29473, 29518, 29515, 6438, 1032, 1401, 4103, 1070, 1086, 1435, 1498, 2181, 1040, 4625, 8195, 29491, 781, 6158, 1211, 781, 781, 6158, 2722, 1291, 4267, 29535, 781, 6158, 2722, 11473, 1042, 1164, 22618, 14612, 2927, 1072, 14572, 1122, 2956, 3995, 2605, 29491, 781, 6158, 2722, 2635, 4267, 29535, 781, 6158, 2722, 1291, 1916, 1909, 1503, 3640, 2851, 2123, 781, 6158, 2722, 4318, 2851, 1093, 6543, 2851, 2097, 1183, 3526, 1070, 1040, 2636, 14612, 2927, 29491, 781, 6158, 2722, 2635, 1916, 29535, 781, 6158, 2722, 1291, 8385, 29535, 781, 6158, 2722, 11058, 29515, 4125, 10593, 1040, 1158, 15706, 1375, 5993, 1070, 10225, 1040, 2927, 29491, 781, 6158, 2722, 2635, 8385, 29535, 781, 6158, 1566, 6523, 11058, 17760, 3424, 29500, 6543, 2851, 4318, 2851, 29499, 29473, 781, 6158, 1139, 781, 18190, 1357, 8036, 29473, 29508, 29515, 6297, 1040, 9984, 1421, 2806, 1070, 1040, 1268, 4595, 4103, 1066, 2645, 1281, 1040, 2956, 1117, 24011, 29491, 781, 18190, 1357, 8036, 29473, 29518, 29515, 1815, 1040, 2956, 1117, 1227, 24011, 29493, 1576, 1040, 3667, 4193, 3464, 1066, 29473, 29549, 29502, 29508, 1703, 27414, 29491, 781, 18190, 1357, 8036, 29473, 29538, 29515, 8432, 1346, 29493, 16987, 1040, 2956, 1066, 1040, 15338, 3652, 1093, 10086, 1054, 1343, 1065, 1040, 3464, 1377, 781, 18190, 1357, 8036, 29473, 29549, 29515, 1815, 24011, 29493, 1802, 1040, 2447, 5754, 2474, 6409, 1065, 1040, 17660, 29491, 781, 6158, 1211, 781, 781, 6158, 2722, 1291, 11491, 29535, 781, 6158, 2722, 1155, 1351, 4191, 1093, 2980, 18063, 2097, 1150, 5626, 1040, 6401, 1066, 1040, 2447, 5754, 2474, 1065, 1040, 17660, 29491, 781, 6158, 2722, 1155, 1268, 4595, 1093, 29503, 1435, 1498, 2097, 27430, 1070, 1086, 1435, 1498, 2075, 1122, 13620, 2956, 21426, 29491, 781, 6158, 2722, 2635, 11491, 29535, 781, 3055, 1211, 781, 29520, 1027, 781, 7994, 11911, 29515, 9557, 29485, 1579, 6650, 781, 9289, 12965, 29491, 6314, 29513, 781, 9289, 8670, 29491, 14909, 29491, 4454, 29513, 781, 9289, 8670, 29491, 14909, 29491, 6543, 29513, 781, 9289, 8670, 29491, 20672, 29491, 4786, 29513, 781, 9289, 2903, 29491, 25399, 29491, 16607, 29513, 781, 9289, 1268, 4595, 29513, 781, 781, 14915, 7121, 7887, 29491, 27053, 781, 29519, 781, 3055, 1357, 1763, 1761, 1695, 1066, 5198, 1040, 8670, 29491, 14909, 29491, 6543, 29491, 7338, 7549, 2076, 4428, 1546, 1342, 3256, 781, 3055, 1566, 1643, 1268, 4595, 27053, 781, 3055, 1139, 781, 6158, 2365, 11025, 12284, 18063, 1351, 4191, 29513, 781, 781, 6158, 2365, 11025, 1086, 1435, 1498, 1268, 4595, 29513, 781, 6158, 1566, 1268, 4595, 27053, 29500, 2980, 18063, 2447, 29493, 1086, 1435, 1498, 29491, 29547, 3009, 3471, 29499, 781, 6158, 1139, 781, 18190, 1351, 4191, 1095, 2447, 29513, 781, 18190, 1268, 4595, 1095, 1401, 1086, 1435, 1498, 29500, 3009, 1112, 781, 6158, 1211, 781, 6158, 1566, 6523, 11058, 17760, 3424, 29500, 6543, 2851, 4318, 2851, 29499, 781, 6158, 1139, 781, 18190, 1281, 2429, 29547, 4595, 29491, 12086, 1421, 29500, 3640, 2851, 1511, 781, 18190, 1139, 781, 2185, 29473, 1357, 4318, 2851, 29491, 3731, 29491, 26401, 29500, 29547, 4595, 29491, 15769, 13379, 26401, 29500, 3640, 2851, 29491, 2980, 29491, 8055, 29491, 2230, 29493, 4318, 2851, 29491, 2980, 29491, 3297, 1325, 1900, 1112, 781, 2185, 29473, 4318, 2851, 29491, 3731, 29491, 20577, 1095, 11733, 26318, 29491, 3906, 29549, 29502, 29508, 2501, 27414, 29513, 781, 2185, 29473, 1372, 29513, 781, 18190, 1211, 781, 781, 781, 781, 18190, 5061, 1351, 4191, 29500, 3640, 2851, 1112, 781, 781, 781, 6158, 1211, 781, 3055, 1211, 781, 781, 781, 29520, 781, 781, 14708, 29600, 29473, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "./mistral_finetuned_v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5276' max='5276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5276/5276 31:25:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.216416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.190983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.179494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.167556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.162889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 33:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': 0.16202066838741302, 'eval_runtime': 2035.1329, 'eval_samples_per_second': 0.429, 'eval_steps_per_second': 0.054, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_from_disk, Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def explode_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Explodes the \"text\" list in each example so that each element becomes its own example.\n",
    "    \"\"\"\n",
    "    new_examples = []\n",
    "    for example in dataset:\n",
    "        # example[\"text\"] is expected to be a list\n",
    "        for t in example[\"text\"]:\n",
    "            new_examples.append({\"text\": t})\n",
    "    return Dataset.from_list(new_examples)\n",
    "def main():\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Mistral_v3_instruct\\\\mistral_finetuned_v4-merged\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    raw_datasets = load_from_disk('E:\\\\fine_tuning\\\\dataset_code_bank')\n",
    "\n",
    "    system_prompt = (\n",
    "        \"\\n**Code Generation Request**\\n\\n\"\n",
    "        \" * Read the provided **Method Descriptions & Summaries**\\n\"\n",
    "        \" * **Complete the Body of Each Block code** based on the corresponding summaries\\n\"\n",
    "        \" * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\\n\"\n",
    "        \" * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\\n\"\n",
    "    )\n",
    "\n",
    "    def chunk_text(text, tokenizer, max_length=1024, stride=512):\n",
    "        \"\"\"\n",
    "        Tokenize the full text and then split it into overlapping chunks.\n",
    "        Each chunk is decoded back to a string.\n",
    "        \"\"\"\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), stride):\n",
    "            chunk_tokens = tokens[i: i + max_length]\n",
    "            chunk_str = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            chunks.append(chunk_str)\n",
    "        return chunks\n",
    "\n",
    "    def preprocess_example(batch):\n",
    "        \"\"\"\n",
    "        Process a batch of examples by creating sliding window chunks.\n",
    "        Each example's text is transformed into a list of chunk strings.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for prompt, response in zip(batch[\"prompt\"], batch[\"response\"]):\n",
    "            if isinstance(prompt, list):\n",
    "                prompt = \" \".join(prompt)\n",
    "            if isinstance(response, list):\n",
    "                response = \" \".join(response)\n",
    "            \n",
    "            full_text = (\n",
    "                \"<s>[INST] System: \" + system_prompt + \"\\n\"\n",
    "                \"User: \" + prompt.strip() + \" [/INST] \\n\"\n",
    "                \"Assistant: \" + response.strip() + \" </s>\"\n",
    "            )\n",
    "            \n",
    "            chunks = chunk_text(full_text, tokenizer, max_length=1024, stride=512)\n",
    "            all_chunks.append(chunks)\n",
    "        \n",
    "        return {\"text\": all_chunks}\n",
    "\n",
    "    # Process examples in batches.\n",
    "    processed_datasets = raw_datasets.map(preprocess_example, batched=True, num_proc=4)\n",
    "\n",
    "    # Explode the \"text\" list into individual examples for each split.\n",
    "    for split in processed_datasets.keys():\n",
    "        processed_datasets[split] = explode_dataset(processed_datasets[split])\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "    # Remove unnecessary columns.\n",
    "    remove_columns = processed_datasets[\"train\"].column_names\n",
    "    tokenized_datasets = processed_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=remove_columns,\n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "    print(\"Train dataset size:\", len(tokenized_datasets[\"train\"]))\n",
    "    print(\"Validation dataset size:\", len(tokenized_datasets[\"validation\"]))\n",
    "    if len(tokenized_datasets[\"validation\"]) > 0:\n",
    "        print(\"A sample validation example (tokenized):\")\n",
    "        print(tokenized_datasets[\"validation\"][0])\n",
    "    else:\n",
    "        print(\"Validation dataset is empty!\")\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "    save_dir = \"./mistral_finetuned_v5\"\n",
    "    \n",
    "    print(save_dir)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_first_step=True,\n",
    "        eval_steps=1000,\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    print(\"Evaluation metrics:\", eval_metrics)\n",
    "\n",
    "    model.save_pretrained(save_dir+\"_lora\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging LoRA adapters with base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cd7d161ed4438ca912702ab4711a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "e:\\fine_tuning\\env\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab88cc38b96441b89fe258ecf58df11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to./mistral_finetuned_v5-merged\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import  PeftModel  # Added PeftModel import\n",
    "\n",
    "def main():\n",
    "    # [Previous code remains identical until after model.save_pretrained()...]\n",
    "\n",
    "    save_dir =  \"./mistral_finetuned_v5\"\n",
    "\n",
    "    # New section: Merge LoRA with base model\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nMerging LoRA adapters with base model...\")\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Mistral_v3_instruct\\\\mistral_finetuned_v4-merged\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    # Reload base model in FP16 (without 4-bit quantization)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter weights\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        save_dir+\"_lora\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Merge and save\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(save_dir+\"-merged\")\n",
    "    tokenizer.save_pretrained(save_dir+\"-merged\")\n",
    "    \n",
    "    print(\"Merged model saved to\"+ save_dir +\"-merged\")\n",
    "    # --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc8c5b3b68b49b1a71a4dd6c4723a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800330b45df848cdae3585d005595daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeBLEU Score for all examples: {'codebleu': 0.2655661612783676, 'ngram_match_score': 0.1040344903212302, 'weighted_ngram_match_score': 0.21692194150853925, 'syntax_match_score': 0.6176635591122844, 'dataflow_match_score': 0.12364465417141646}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "def main():\n",
    "    # Specify the model path from Hugging Face\n",
    "    model_name = \"E:\\\\fine_tuning\\\\Mistral_v3_instruct\\\\mistral_finetuned_v5-merged\"\n",
    "\n",
    "    # Load the tokenizer and set the padding token (using eos_token if not already set)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Adjust the tokenizer padding side to \"right\"\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # Load the base model in 4-bit mode using bitsandbytes for memory efficiency\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,      # requires bitsandbytes; lowers memory usage\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # Additional model configurations\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.config.window = 256\n",
    "\n",
    "    # Load test dataset and select only 10 examples\n",
    "    full_test_dataset = load_from_disk(\"E:\\\\fine_tuning\\\\dataset_code_bank\")[\"test\"]\n",
    "    # test_dataset = full_test_dataset.select(range(10))\n",
    "    test_dataset = full_test_dataset\n",
    "    # Define system prompt\n",
    "    system_prompt = '''\n",
    "        **Code Generation Request** \n",
    "\n",
    "        * Read the provided **Method Descriptions & Summaries**\n",
    "        * **Complete the Body of Each Block code** based on the corresponding summaries\n",
    "        * **Format Requirement:** Wrap all generated **Code Blocks** in triple backticks (```) for enhanced readability\n",
    "        * **Delivery Note:** Provide only the completed **Code Blocks** without explanatory comments or text\n",
    "    '''\n",
    "\n",
    "    # Preprocess function to combine the system prompt and user prompt into a single input string\n",
    "    def preprocess_example(example):\n",
    "        example[\"text\"] = (\n",
    "            \"<s>[INST] \"\n",
    "            \"System: \" + system_prompt + \"\\n\"\n",
    "            \"User: \" + example[\"prompt\"].strip() + \" [/INST] \\n\"\n",
    "            \"Assistant: \"\n",
    "        )\n",
    "        return example\n",
    "\n",
    "    # Preprocess test dataset\n",
    "    processed_test_dataset = test_dataset.map(preprocess_example)\n",
    "\n",
    "    # Tokenization function (using truncation to 2048 tokens for compatibility with fine-tuning)\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, max_length=2048, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_test_dataset = processed_test_dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=test_dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Generate predictions using max_new_tokens instead of max_length\n",
    "    def generate_code(example):\n",
    "        input_ids = tokenizer(example[\"text\"], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,  # Generate 512 new tokens after the prompt\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        example[\"generated_code\"] = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return example\n",
    "\n",
    "    generated_results = tokenized_test_dataset.map(generate_code)\n",
    "\n",
    "    # Evaluate with CodeBLEU on these 10 examples\n",
    "    references = [ex[\"response\"] for ex in test_dataset]\n",
    "    hypotheses = [ex[\"generated_code\"] for ex in generated_results]\n",
    "    # Compute CodeBLEU score\n",
    "    codebleu_score = calc_codebleu(\n",
    "        references,        # list of reference code (or list of lists if there are multiple references)\n",
    "        hypotheses,        # list of candidate code\n",
    "        lang=\"c_sharp\",           # specify the programming language\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),  # weights for n-gram, weighted n-gram, syntax, and data-flow matches\n",
    "        tokenizer=None           # if None, the default string split is used\n",
    "    )\n",
    "\n",
    "    print(\"CodeBLEU Score for all examples:\", codebleu_score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
